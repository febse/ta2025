{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Text Summarization\n",
    "\n",
    "- Split the document into sentences (sentence tokenization)\n",
    "- Assign a score to each sentence\n",
    "- Pick the top N sentences based on the scores\n",
    "\n",
    "- Score = Average (non-zero TF-IDF of words in the sentence) \n",
    "    - Unimportant words have smaller values in the TF-IDF matrix\n",
    "    - Important words appearing more often in the sentence will have an even higher score.\n",
    "    - Average instead of just summing to avoid bias towards longer sentences. \n",
    "    - Non-zero elements because the TF-IDF is very sparse and don't want to choose based on variety of words\n",
    "\n",
    "- TextRank score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ArticleId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5adfc779-2aa6-4fb1-809a-46adc9913020",
       "rows": [
        [
         "0",
         "1833",
         "worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.",
         "business"
        ],
        [
         "1",
         "154",
         "german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.",
         "business"
        ],
        [
         "2",
         "1101",
         "bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.",
         "business"
        ],
        [
         "3",
         "1976",
         "lifestyle  governs mobile choice  faster  better or funkier hardware alone is not going to help phone firms sell more handsets  research suggests.  instead  phone firms keen to get more out of their customers should not just be pushing the technology for its own sake. consumers are far more interested in how handsets fit in with their lifestyle than they are in screen size  onboard memory or the chip inside  shows an in-depth study by handset maker ericsson.  historically in the industry there has been too much focus on using technology   said dr michael bjorn  senior advisor on mobile media at ericsson s consumer and enterprise lab.  we have to stop saying that these technologies will change their lives   he said.  we should try to speak to consumers in their own language and help them see how it fits in with what they are doing   he told the bbc news website.  for the study  ericsson interviewed 14 000 mobile phone owners on the ways they use their phone.  people s habits remain the same   said dr bjorn.  they just move the activity into the mobile phone as it s a much more convenient way to do it.   one good example of this was diary-writing among younger people  he said. while diaries have always been popular  a mobile phone -- especially one equipped with a camera -- helps them keep it in a different form. youngsters  use of text messages also reflects their desire to chat and keep in contact with friends and again just lets them do it in a slightly changed way. dr bjorn said that although consumers do what they always did but use a phone to do it  the sheer variety of what the new handset technologies make possible does gradually drive new habits and lifestyles. ericsson s research has shown that consumers divide into different  tribes  that use phones in different ways. dr bjorn said groups dubbed  pioneers  and  materialists  were most interested in trying new things and were behind the start of many trends in phone use.  for instance   he said   older people are using sms much more than they did five years ago.  this was because younger users  often the children of ageing mobile owners  encouraged older people to try it so they could keep in touch.  another factor governing the speed of change in mobile phone use was the simple speed with which new devices are bought by pioneers and materialists. only when about 25% of people have handsets with new innovations on them  such as cameras  can consumers stop worrying that if they send a picture message the person at the other end will be able to see it.  once this significant number of users is passed  use of new innovations tends to take off. dr bjorn said that early reports of camera phone usage in japan seemed to imply that the innovation was going to be a flop. however  he said  now 45% of the japanese people ericsson questioned use their camera phone at least once a month. in 2003 the figure was 29%. similarly  across europe the numbers of people taking snaps with cameras is starting to rise. in 2003 only 4% of the people in the uk took a phonecam snap at least once a month. now the figure is 14%. similar rises have been seen in many other european nations. dr bjorn said that people also used their camera phones in very different ways to film and even digital cameras.  usage patterns for digital cameras are almost exactly replacing usage patterns for analogue cameras   he said. digital cameras tend to be used on significant events such as weddings  holidays and birthdays. by contrast  he said  camera phones were being used much more to capture a moment and were being woven into everyday life.",
         "tech"
        ],
        [
         "4",
         "917",
         "enron bosses in $168m payout eighteen former enron directors have agreed a $168m (£89m) settlement deal in a shareholder lawsuit over the collapse of the energy firm.  leading plaintiff  the university of california  announced the news  adding that 10 of the former directors will pay $13m from their own pockets. the settlement will be put to the courts for approval next week. enron went bankrupt in 2001 after it emerged it had hidden hundreds of millions of dollars in debt.  before its collapse  the firm was the seventh biggest public us company by revenue. its demise sent shockwaves through financial markets and dented investor confidence in corporate america.   the settlement is very significant in holding these outside directors at least partially personally responsible   william lerach  the lawyer leading the class action suit against enron  said.  hopefully  this will help send a message to corporate boardrooms of the importance of directors performing their legal duties   he added. under the terms of the $168m settlement - $155m of which will be covered by insurance - none of the 18 former directors will admit any wrongdoing. the deal is the fourth major settlement negotiated by lawyers who filed a class action on behalf of enron s shareholders almost three years ago. so far  including the latest deal  just under $500m (£378.8m) has been retrieved for investors.  however  the latest deal does not include former enron chief executives ken lay and jeff skilling. both men are facing criminal charges for their alleged misconduct in the run up to the firm s collapse. neither does it cover andrew fastow  who has pleaded guilty to taking part in an illegal conspiracy while he was chief financial officer at the group. enron s shareholders are still seeking damages from a long list of other big name defendants including the financial institutions jp morgan chase  citigroup  merrill lynch and credit suisse first boston. the university of california said the trial in the case is scheduled to begin in october 2006. it joined the lawsuit in december 2001alleging  massive insider trading  and fraud  claiming it had lost $145m on its investments in the company.",
         "business"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/BBC%20News%20Train.csv.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uk coal plunges into deeper loss shares in uk coal have fallen after\n",
      "the mining group reported losses had deepened to £51.6m in 2004 from\n",
      "£1.2m.  the uk s biggest coal producer blamed geological problems\n",
      "industrial action and  operating flaws  at its deep mines for its\n",
      "worsening fortunes.  the south yorkshire company  led by new chief\n",
      "executive gerry spindler  said it hoped to return to profit in 2006.\n",
      "in early trade on thursday  its shares were down 10% at 119 pence.  uk\n",
      "coal said it was making  significant progress  in shaking up the\n",
      "business.  it had introduced new wage structures  a new daily\n",
      "maintenance regime for machinery at its mines and methods to continue\n",
      "mining in adverse conditions.  the company said these actions should\n",
      "significantly uplift earnings . it expected 2005 to be a  transitional\n",
      "year  and to return to profitability in 2006.  the recent rise in coal\n",
      "prices has failed to benefit the company as most of its output had\n",
      "already been sold  it said.  total production costs were £1.30 per\n",
      "gigajoule  uk coal said  but the average selling price was just £1.18\n",
      "per gigajoule.  we have a long journey ahead to fix these issues.  we\n",
      "continue to make progress and great strides have already been made\n",
      "said mr spindler.  uk coal operates 15 deep and surface mines across\n",
      "nottinghamshire  derbyshire  leicestershire  yorkshire  the west\n",
      "midlands  northumberland and durham.\n"
     ]
    }
   ],
   "source": [
    "doc = df.iloc[12]\n",
    "\n",
    "print(textwrap.fill(doc[\"Text\"], replace_whitespace=False, fix_sentence_endings=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uk coal plunges into deeper loss shares in uk coal have fallen after the mining group reported losses had deepened to £51.6m in 2004 from £1.2m.',\n",
       " 'the uk s biggest coal producer blamed geological problems  industrial action and  operating flaws  at its deep mines for its worsening fortunes.',\n",
       " 'the south yorkshire company  led by new chief executive gerry spindler  said it hoped to return to profit in 2006. in early trade on thursday  its shares were down 10% at 119 pence.',\n",
       " 'uk coal said it was making  significant progress  in shaking up the business.',\n",
       " 'it had introduced new wage structures  a new daily maintenance regime for machinery at its mines and methods to continue mining in adverse conditions.',\n",
       " 'the company said these actions should  significantly uplift earnings .',\n",
       " 'it expected 2005 to be a  transitional year  and to return to profitability in 2006.  the recent rise in coal prices has failed to benefit the company as most of its output had already been sold  it said.',\n",
       " 'total production costs were £1.30 per gigajoule  uk coal said  but the average selling price was just £1.18 per gigajoule.',\n",
       " 'we have a long journey ahead to fix these issues.',\n",
       " 'we continue to make progress and great strides have already been made   said mr spindler.',\n",
       " 'uk coal operates 15 deep and surface mines across nottinghamshire  derbyshire  leicestershire  yorkshire  the west midlands  northumberland and durham.']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sents = sent_tokenize(doc[\"Text\"])\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "X = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 113)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-wise Mean of Non-zero Elements:\n",
      " [0.24924015 0.25482022 0.21658328 0.34347024 0.26179554 0.40054402\n",
      " 0.23270168 0.25594703 0.4472136  0.31275452 0.24686998]\n"
     ]
    }
   ],
   "source": [
    "X = X.toarray()\n",
    "scores = np.array([np.mean(row[row != 0]) for row in X])\n",
    "print(\"Row-wise Mean of Non-zero Elements:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top sentences:\n",
      "\n",
      "0.45: we have a long journey ahead to fix these issues.\n",
      "0.40: the company said these actions should  significantly uplift earnings .\n",
      "0.34: uk coal said it was making  significant progress  in shaking up the business.\n",
      "0.31: we continue to make progress and great strides have already been made   said mr spindler.\n",
      "0.26: it had introduced new wage structures  a new daily maintenance regime for machinery at its mines and methods to continue mining in adverse conditions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Top sentences:\\n\")\n",
    "\n",
    "for i in sort_idx[:5]:\n",
    "  print(f\"%.2f: %s\" % (scores[i], sents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank\n",
    "\n",
    "TextRank is an unsupervised keyword and sentence extraction algorithm that is inspired by PageRank, the Google page ranking algorithm in the 1990s and 2000s. \n",
    "\n",
    "Before we can see how to apply TextRank to text summarization, we need to understand how PageRank works.\n",
    "PageRank is a link analysis algorithm that assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "graph TD\n",
    "    A[Page A] -->|Link| B[Page B]\n",
    "    A -->|Link| C[Page C]\n",
    "    B -->|Link| D[Page D]\n",
    "    C -->|Link| D\n",
    "    D -->|Link| A\n",
    "    D -->|Link| E[Page E]\n",
    "    E -->|Link| B\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a walk through the web. We start at a random webpage and follow a random link on that page. On the next page, we again follow a randomly chosen link. We keep doing this for a long time. We can ask a question: what is the probability that we end up on a certain page?\n",
    "\n",
    "We can view this walk as a Markov chain. Say that all pages are $n$ and let's assume that we can reach any page from any other page (though not \n",
    "with the same probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 144395 characters\n",
      "Cleaned text length: 107693 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VCVCVCVCCVCCVCVCVCCVCCVCCVCCCCCVCVCCVCCVCCCCCCCCVCCVCVCVCCCCVCVCCVCCVCVVCVCVCVCCVCVCCVCCCVCVCCVCCCVC'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "\n",
    "print(f\"Text length: {len(alice)} characters\")\n",
    "\n",
    "# Remove non-word characters and spaces, lowercase\n",
    "\n",
    "import re\n",
    "alice_clean = re.sub(r'\\s+', '', re.sub(r'[^\\w\\s]', '', alice)).lower()\n",
    "\n",
    "print(f\"Cleaned text length: {len(alice_clean)} characters\")\n",
    "\n",
    "# Convert to a sequence of consonants and vowels\n",
    "\n",
    "alice_cv = re.sub(r'[aeiou]', 'V', re.sub(r'[^aeiou\\s]', 'C', alice_clean))\n",
    "alice_cv[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'C': 66212, 'V': 41481})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the frequencies of vowels and consonants\n",
    "\n",
    "from collections import Counter\n",
    "cv_counts = Counter(alice_cv)\n",
    "cv_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(V): 0.3852, P(C): 0.6148\n"
     ]
    }
   ],
   "source": [
    "P_V = cv_counts['V'] / (cv_counts['V'] + cv_counts['C'])  # P(V)\n",
    "P_C = cv_counts['C'] / (cv_counts['V'] + cv_counts['C'])  # P(C)\n",
    "\n",
    "print(f\"P(V): {P_V:.4f}, P(C): {P_C:.4f}\")\n",
    "# Count the frequencies of VV, CC, VC, CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'VC': 34838, 'CV': 34837, 'CC': 31374, 'VV': 6643})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the frequencies of VV, CC, VC, CV\n",
    "\n",
    "cv_bigrams = [alice_cv[i:i+2] for i in range(len(alice_cv)-1)]\n",
    "bigram_counts = Counter(cv_bigrams)\n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(V->V|V): 0.16, P(V->C|V): 0.84\n",
      "P(C->C|C): 0.47, P(C->V|C): 0.53\n"
     ]
    }
   ],
   "source": [
    "V_to_C = 34838 / (34838 + 6643)  # P(V->V | V)\n",
    "V_to_V = 6643 / (34838 + 6643)   # P(V->C | V)\n",
    "\n",
    "C_to_C = 31374 / (31374 + 34837) # P(C->C | C)\n",
    "C_to_V = 34837 / (31374 + 34837) # P(C->V | C)\n",
    "\n",
    "print(f\"P(V->V|V): {V_to_V:.2}, P(V->C|V): {V_to_C:.2}\")\n",
    "print(f\"P(C->C|C): {C_to_C:.2}, P(C->V|C): {C_to_V:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequencies of vowels and consonants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now track the states of the Markov chain as we jump between vowels and consonants in \"Alice's Adventures in Wonderland\".\n",
    "\n",
    "We perform a series of transitions between two states: Vowel (V) and Consonant (C). Let's denote the state we are in at step $t$ as $S_t$.\n",
    "So $S_t = V$ means that at step $t$ we are at a vowel, and $S_t = C$ means we are at a consonant.\n",
    "\n",
    "- We start by picking a state at random, say with equal probability:\n",
    "  - $P(S_0 = V) = 0.5$\n",
    "  - $P(S_0 = C) = 0.5$\n",
    "- Next, we jump to the next state based on the transition probabilities:\n",
    "  - $P(S_{t+1} = V | S_t = V) = 0.84$ (probability of going from vowel to vowel)\n",
    "  - $P(S_{t+1} = C | S_t = V) = 0.16$ (probability of going from vowel to consonant)\n",
    "  - $P(S_{t+1} = C | S_t = C) = 0.47$ (probability of going from consonant to consonant)\n",
    "  - $P(S_{t+1} = V | S_t = C) = 0.53$ (probability of going from consonant to vowel)\n",
    "  \n",
    "\n",
    "What are the probabilities of landing at a vowel/consonant in step 1? We can use the law of total probability:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(S_1 = v) & = P(S_1 = v | S_0 = v) P(S_0 = v) + P(S_1 = v | S_0 = c) P(S_0 = c) \\\\\n",
    "P(S_1 = c) & = P(S_1 = c | S_0 = v) P(S_0 = v) + P(S_1 = c | S_0 = c) P(S_0 = c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the following it will be convenient to express the probabilities as a vector and the transition probabilities as a matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{p_t} = \\begin{pmatrix} P(S_t = v) & P(S_t = c) \\end{pmatrix} , \\quad \\mathbf{T} = \\begin{pmatrix} P(S_{t+1} = v | S_t = v) & P(S_{t+1} = c | S_t = v) \\\\ P(S_{t+1} = v | S_t = c) & P(S_{t+1} = c | S_t = c) \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now we can express the probabilities for the two states at step $t+1$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{p_{t+1}} = \\mathbf{p_t} \\mathbf{T}\n",
    "$$\n",
    "\n",
    "We can recursively apply this to get the probabilities at step $t$ starting from the initial probabilities $\\mathbf{p_0}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{p_t} & = \\mathbf{p_{t - 1}} \\mathbf{T} \\mathbf{T} \\\\\n",
    "& = \\mathbf{p_{t - 2}} \\mathbf{T} \\mathbf{T} \\mathbf{T} \\\\\n",
    "& \\ \\, \\vdots \\\\\n",
    "              & = \\mathbf{p_0} \\mathbf{T}^t\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "But is there a steady state distribution $\\mathbf{p^*}$ such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{p^*} = \\mathbf{p^*} \\mathbf{T}\n",
    "$$\n",
    "\n",
    "The answer is provided by the Perron-Frobenius theorem, which states that an ergodic Markov chain has a unique stationary distribution. To be ergodic, the Markov chain must be irreducible (it is possible to get to any state from any state) and aperiodic (the system does not get trapped in cycles).\n",
    "\n",
    "How do we compute $\\mathbf{p^*}$?\n",
    "\n",
    "The last equation can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\mathbf{p^*} (\\mathbf{T} - \\mathbf{I}) = 0\n",
    "$$\n",
    "\n",
    "which is an eigenvalue problem. The stationary distribution $\\mathbf{p^*}$ is the left eigenvector of the transition matrix $\\mathbf{T}$ corresponding to the eigenvalue 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix:\n",
      " [[0.16014561 0.83985439]\n",
      " [0.52615124 0.47384876]]\n",
      "After step 1: P(V)=0.3431, P(C)=0.6569\n",
      "After step 2: P(V)=0.4006, P(C)=0.5994\n",
      "After step 3: P(V)=0.3795, P(C)=0.6205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p0 = np.array([0.5, 0.5])\n",
    "\n",
    "T = np.array([[V_to_V, V_to_C],\n",
    "              [C_to_V, C_to_C]])\n",
    "\n",
    "print(\"Transition Matrix:\\n\", T)\n",
    "\n",
    "def markov_step(p, T):\n",
    "    return np.dot(p, T)\n",
    "\n",
    "for step in range(3):\n",
    "    p0 = markov_step(p0, T)\n",
    "    print(f\"After step {step+1}: P(V)={p0[0]:.4f}, P(C)={p0[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [-0.36600563  1.        ]\n",
      "Eigenvectors:\n",
      " [[-0.70710678 -0.53090001]\n",
      " [ 0.70710678 -0.84743447]]\n",
      "\n",
      "Stationary distribution: [0.38517502 0.61482498]\n",
      "P(V) = 0.3852, P(C) = 0.6148\n"
     ]
    }
   ],
   "source": [
    "# Compute the steady-state distribution\n",
    "eigenvalues, eigenvectors = np.linalg.eig(T.T)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# Find the eigenvector corresponding to eigenvalue 1\n",
    "eigenvector_to_ev_1 = eigenvectors[:, 1]\n",
    "\n",
    "# Normalize to get probabilities\n",
    "stationary_dist = eigenvector_to_ev_1 / eigenvector_to_ev_1.sum()\n",
    "\n",
    "print(f\"\\nStationary distribution: {stationary_dist}\")\n",
    "print(f\"P(V) = {stationary_dist[0]:.4f}, P(C) = {stationary_dist[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "## Numpy and Eigenvalues\n",
    "\n",
    "You may have noticed that we called `np.linalg.eig` with the transpose of the transition matrix.\n",
    "This is because `np.linalg.eig` computes the right eigenvectors of a matrix, i.e., it solves the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}$ is a column vector. Your $\\mathbf{p}_t$ vector is a row vector, so to find\n",
    "the correct eigenvector we need to transpose the matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T \\mathbf{A} = \\lambda \\mathbf{v}^T\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextRank algorithm scores sentences based on the stationary distribution of a Markov chain. Instead of webpages we have sentences. There are no real transition probabilities between sentences, but we can use the cosine similarity between the sentence representations in the TF-IDF space as a proxy.\n",
    "\n",
    "Let's implement it as an exercise.\n",
    "\n",
    "- Compute the TF-IDF matrix of the sentences\n",
    "- Compute the cosine similarity matrix\n",
    "- Normalize the cosine similarity matrix to get the transition matrix\n",
    "- Smooth the transition matrix\n",
    "- Compute the stationary distribution\n",
    "- Rank the sentences based on the stationary distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (11, 113)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.27769841 0.         0.        ]\n",
      " [0.23406501 0.23406501 0.         ... 0.         0.         0.20007025]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.27019104 ... 0.         0.         0.23094946]]\n"
     ]
    }
   ],
   "source": [
    "# We already have th TF-IDF matrix X from before\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we compute the cosine similarity matrix\n",
    "# between each pair of sentences\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(X)\n",
    "\n",
    "# print(\"Cosine similarity shape:\", cosine_sim_matrix.shape)\n",
    "# print(\"Cosine Similarity Matrix:\\n\", cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This matrix is no transition matrix, though \n",
    "# as the rows do not sum to 1.\n",
    "\n",
    "T = cosine_sim_matrix / cosine_sim_matrix.sum(axis=1, keepdims=True)\n",
    "T.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PageRank algorithm applies smoothing to the transition matrix as in practice it is not possible for every page to link to every other page.\n",
    "\n",
    "The smoothing is done by adding a damping factor $d$ to the transition matrix\n",
    "\n",
    "$$\n",
    "P = \\alpha T + (1-\\alpha) E, \\quad \\alpha \\in [0, 1]\n",
    "$$\n",
    "\n",
    "where $T$ is the original transition matrix and $E$ is a matrix with all elements equal to $1/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "E = np.ones_like(T) / T.shape[0]\n",
    "P = alpha * T + (1 - alpha) * E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      " [1.         0.1        0.048277   0.08351595 0.07913968 0.05467085\n",
      " 0.05919873 0.06090568 0.06362465 0.07016773 0.06760339]\n"
     ]
    }
   ],
   "source": [
    "# Now we need to find the stationary distribution of P\n",
    "\n",
    "evals, evecs = np.linalg.eig(P.T)\n",
    "print(\"Eigenvalues:\\n\", evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3018511 , 0.30110402, 0.30234851, 0.30366103, 0.3004944 ,\n",
       "       0.30047697, 0.30178986, 0.30069743, 0.30151006, 0.3012753 ,\n",
       "       0.30140199])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_stationary = evecs[:,0]\n",
    "ev_stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09101192, 0.09078666, 0.09116189, 0.09155763, 0.09060286,\n",
       "       0.0905976 , 0.09099345, 0.09066407, 0.09090909, 0.09083831,\n",
       "       0.09087651])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationary_dist = ev_stationary / ev_stationary.sum()\n",
    "stationary_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
