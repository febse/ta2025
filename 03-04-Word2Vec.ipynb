{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afe875a",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "In the previous section, we have explored a model that predicts a word based on the word preceding it (the bi-gram model).\n",
    "It used a logistic multi-class logistic regression to predict the next word.\n",
    "\n",
    "For a single bi-gram, the input to the model is a one-hot encoded vector representing the preceding word (indicator vector with a 1 at the index of the word and 0s elsewhere).\n",
    "The output of the model is a probability distribution over the vocabulary, meaning a vector of probabilities for each word in the vocabulary.\n",
    "\n",
    "The training of the model adjusts the weights of the logistic regression to maximize the log-likelihood (minimize the negative log-likelihood or cross-entropy loss) of the observed word pairs in the training data.\n",
    "The training uses the actual distribution of the next word which is again a one-hot encoded vector that assigns all the probability mass to the actual next word.\n",
    "\n",
    "Here we want to address one limitation of this approach and that is the number of parameters.\n",
    "\n",
    "$$\n",
    "z = W x + b = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1V} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2V} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{D1} & w_{D2} & \\dots & w_{VV}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{V}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{V}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "z_{1} \\\\\n",
    "z_{2} \\\\\n",
    "\\vdots \\\\\n",
    "z_{V}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38a21c",
   "metadata": {},
   "source": [
    "![Skip-gram Model](./figures/skip-gram-architecture.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860c507",
   "metadata": {},
   "source": [
    "\n",
    "Where $V$ is the vocabulary size. One way to reduce the number of parameters is to introduce a hidden layer with a smaller dimension $D$ (where $D << V$).\n",
    "\n",
    "$$\n",
    "h = W_{h} x + b_{h} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1V} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2V} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{D1} & w_{D2} & \\dots & w_{DV}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{V}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{D}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{D}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We will set the activation function of this hidden layer to be the identity function (we will explain this choice later).\n",
    "\n",
    "$$\n",
    "z = W_{o} h + b_{o} = \\begin{bmatrix}\n",
    "w^{o}_{11} & w^{o}_{12} & \\dots & w^{o}_{1D} \\\\\n",
    "w^{o}_{21} & w^{o}_{22} & \\dots & w^{o}_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{o}_{V1} & w^{o}_{V2} & \\dots & w^{o}_{VD}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{D}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "b^{o}_1 \\\\\n",
    "b^{o}_2 \\\\\n",
    "\\vdots \\\\\n",
    "b^{o}_V\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "z_{1} \\\\\n",
    "z_{2} \\\\\n",
    "\\vdots \\\\\n",
    "z_{V}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The prediction is again the result of applying the softmax function to the output layer:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z)\n",
    "$$\n",
    "\n",
    "Let's now count the number of parameters in this model:\n",
    "\n",
    "- We have the weights mapping the input layer to the hidden layer: $W_{h}$ with $D \\times V$ parameters.\n",
    "- We have the biases of the hidden layer: $b_{h}$ with $D$ parameters.\n",
    "- We have the weights mapping the hidden layer to the output layer: $W_{o}$ with $V \\times D$ parameters.\n",
    "- We have the biases of the output layer: $b_{o}$ with $V$ parameters.\n",
    "\n",
    "So in total, we have: $D \\times V + D + V \\times D + V = 2 \\times D \\times V + D + V$ parameters. With a vocabulary size of *10,000* and a hidden layer size of *100*, we have approximately 2,001,000 parameters.\n",
    "\n",
    "Compare it to the number of parameters in the logistic regression version which is $V \\times V + V = 100,010,000$ parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87a87b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters with the hidden layer: 2,010,100\n",
      "Number of parameters of the logistic regression model: 100,010,000\n"
     ]
    }
   ],
   "source": [
    "V = 10_000\n",
    "D = 100\n",
    "\n",
    "print(f\"Number of parameters with the hidden layer: {2 * V * D + D + V:,}\")\n",
    "print(f\"Number of parameters of the logistic regression model: {V * V + V:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7100a",
   "metadata": {},
   "source": [
    "Let's get rid of the biases for simplicity. Without them, the equations for the two layers become:\n",
    "\n",
    "$$\n",
    "h = W_{h} x = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1V} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2V} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{D1} & w_{D2} & \\dots & w_{DV}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\ \n",
    "x_{V}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{D}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "z = W_{o} h = \\begin{bmatrix}\n",
    "w^{o}_{11} & w^{o}_{12} & \\dots & w^{o}_{1D} \\\\\n",
    "w^{o}_{21} & w^{o}_{22} & \\dots & w^{o}_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{o}_{V1} & w^{o}_{V2} & \\dots & w^{o}_{VD}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{D}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "z_{1} \\\\\n",
    "z_{2} \\\\\n",
    "\\vdots \\\\\n",
    "z_{V}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c6a28",
   "metadata": {},
   "source": [
    "\n",
    "Remember that the input $x$ is a one-hot encoded vector, so the matrix-vector multiplication in the first layer simply selects the column of $W_{h}$ corresponding to the index of the word represented by $x$.\n",
    "\n",
    "The output layer for word $i$ can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_{1} \\\\\n",
    "z_{2} \\\\\n",
    "\\vdots \\\\\n",
    "z_{V}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w^{o}_{11} & w^{o}_{12} & \\dots & w^{o}_{1D} \\\\\n",
    "w^{o}_{21} & w^{o}_{22} & \\dots & w^{o}_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{o}_{V1} & w^{o}_{V2} & \\dots & w^{o}_{VD}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1i} \\\\\n",
    "w_{2i} \\\\\n",
    "\\vdots \\\\\n",
    "w_{Di}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w^{o}_{1} \\cdot w_{i} \\\\\n",
    "w^{o}_{2} \\cdot w_{i} \\\\\n",
    "\\vdots \\\\\n",
    "w^{o}_{V} \\cdot w_{i}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w^{o}_{11} w_{1i} + w^{o}_{12} w_{2i} + \\dots + w^{o}_{1D} w_{Di} \\\\\n",
    "w^{o}_{21} w_{1i} + w^{o}_{22} w_{2i} + \\dots + w^{o}_{2D} w_{Di} \\\\\n",
    "\\vdots \\\\\n",
    "w^{o}_{V1} w_{1i} + w^{o}_{V2} w_{2i} + \\dots + w^{o}_{VD} w_{Di}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The prediction is still obtained by applying the softmax function to the output layer:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z)\n",
    "$$\n",
    "\n",
    "Let's focus on what we are going to optimize during training. The objective is to minimize the cross-entropy loss between the predicted distribution $\\hat{y}$ and the actual distribution $y$ (one-hot encoded vector of the actual next word)\n",
    "but this time we have two sets of parameters: a vector representing each word in the hidden layer and a vector representing each word in the output layer. Let's write the softmax function for a specific word $i$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{j} = \\frac{\\exp(w^{o}_{j} \\cdot w_{i})}{\\sum_{k=1}^{V} \\exp(w^{o}_{k} \\cdot w_{i})}\n",
    "$$\n",
    "\n",
    "and the log-likelihood for the actual next word $y$:\n",
    "\n",
    "$$\n",
    "l = \\sum_{j=1}^{V} y_{j} \\log(\\hat{y}_{j})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b38fd4",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's see a very simple example with just four words in the vocabulary: **\"Alice\", \"was\", \"getting\", \"tired\"**, a hidden layer of size 2, and the bi-grams **(\"Alice\", \"was\")**, **(\"was\", \"getting\")**, **(\"getting\", \"tired\")**. To not get confused with the indices, we will assign each word an index as follows:\n",
    "\n",
    "| Word     | Index (Math) | Index (Numpy) |\n",
    "|----------|-------|-------------|\n",
    "| Alice    | 1     | 0           |\n",
    "| getting  | 2     | 1          |\n",
    "| tired    | 3     | 2          |\n",
    "| was      | 4     | 3           |\n",
    "\n",
    "The first weight matrix $W_{h}$ (input to hidden) will have the following shape: (2, 4) and the second weight matrix $W_{o}$ (hidden to output) will have the shape: (4, 2).\n",
    "\n",
    "$$\n",
    "W_{h} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "W_{o} = \\begin{bmatrix}\n",
    "w^{o}_{11} & w^{o}_{12} \\\\\n",
    "w^{o}_{21} & w^{o}_{22} \\\\\n",
    "w^{o}_{31} & w^{o}_{32} \\\\\n",
    "w^{o}_{41} & w^{o}_{42}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d12ec346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh:\n",
      "[[-0.34889445  0.98370343  0.58092283  0.07028444]\n",
      " [ 0.77753268  0.58195875  1.47179053  1.66318101]]\n",
      "\n",
      "Wo:\n",
      "[[-0.26117712 -0.68867681]\n",
      " [-0.69492326  1.94042346]\n",
      " [ 1.80541519  0.45631385]\n",
      " [-0.57481204  0.1141805 ]]\n",
      "\n",
      "Wh shape: (2, 4)\n",
      "Wo shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = 4\n",
    "D = 2\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "Wh = np.random.randn(D, V)\n",
    "Wo = np.random.randn(V, D)\n",
    "\n",
    "print(\"Wh:\")\n",
    "print(Wh)\n",
    "\n",
    "print(\"\\nWo:\")\n",
    "print(Wo)\n",
    "print(\"\\nWh shape:\", Wh.shape)\n",
    "print(\"Wo shape:\", Wo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12031b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"Alice\", \"getting\", \"tired\", \"was\"]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(Wh[0, i], Wh[1, i], marker='o')\n",
    "    plt.text(Wh[0, i] + 0.02, Wh[1, i] + 0.02, word, fontsize=12)\n",
    "\n",
    "plt.xlabel(\"Hidden dimension 1\")\n",
    "plt.ylabel(\"Hidden dimension 2\")\n",
    "plt.title(\"Word vectors in hidden space\")\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c7b352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_alice:\n",
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# For the first word (Alice, at this case index 0 in the vocabulary in numpy)\n",
    "\n",
    "x_alice = np.zeros(V)\n",
    "x_alice[0] = 1\n",
    "\n",
    "print(\"\\nx_alice:\")\n",
    "print(x_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dac44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h:\n",
      "[-0.34889445  0.77753268]\n",
      "\n",
      "z:\n",
      "[-0.44434548  1.75119752 -0.27510041  0.2893278 ]\n",
      "\n",
      "y_hat:\n",
      "[0.07546027 0.6780014  0.08937593 0.1571624 ]\n",
      "\n",
      "The actual word following 'Alice' is 'was', which is index 4 in the vocabulary (3 in numpy).\n",
      "\n",
      "y_was:\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "h = Wh @ x_alice\n",
    "\n",
    "print(\"\\nh:\")\n",
    "print(h)\n",
    "\n",
    "z = Wo.dot(h)\n",
    "print(\"\\nz:\")\n",
    "print(z)\n",
    "\n",
    "y_hat = np.exp(z) / np.sum(np.exp(z))\n",
    "print(\"\\ny_hat:\")\n",
    "print(y_hat)\n",
    "\n",
    "print(\"\\nThe actual word following 'Alice' is 'was', which is index 4 in the vocabulary (3 in numpy).\")\n",
    "\n",
    "y_was = np.zeros(V)\n",
    "y_was[3] = 1\n",
    "\n",
    "print(\"\\ny_was:\")\n",
    "print(y_was)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d76dac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss:\n",
      "1.8504756415153232\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the loss\n",
    "\n",
    "loss = -np.log(y_hat).dot(y_was)\n",
    "print(\"\\nLoss:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "637d6fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.8504756415153232)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the same as np.log(y_hat[3])\n",
    "\n",
    "-np.log(y_hat[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc9ac2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Think about when the loss will be as small as possible.\n",
    "# If the model outputs a prediction of 1.0 for the correct word,\n",
    "\n",
    "# the loss will be -log(1.0) = 0, which is the smallest possible value for the loss.\n",
    "\n",
    "np.log(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903dbc9",
   "metadata": {},
   "source": [
    "## Gradient Descent Updates\n",
    "\n",
    "We repeat the model equations here for convenience:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h & = W_{h} x \\\\\n",
    "z & = W_{o} h \\\\\n",
    "\\hat{y} & = \\text{softmax}(z) \\\\\n",
    "L & = -\\sum_{i=1}^{V} y_{i} \\log(\\hat{y}_{i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here we present the gradient descent update equations without derivation. For a detailed derivation, please refer to the last section of this chapter.\n",
    "\n",
    "\n",
    "First define the prediction error vector:\n",
    "\n",
    "$$\n",
    "\\delta = \\underbrace{\\hat{y} - y}_{V \\times 1}\n",
    "$$\n",
    "\n",
    "**Output Layer Weights (`W_o`, shape VÃ—D):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial W_{o}}}_{V \\times D} = \\underbrace{(\\hat{y} - y)}_{V \\times 1} \\, \\underbrace{h^{\\top}}_{1 \\times D}\n",
    "$$\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\n",
    "W_{o} \\leftarrow W_{o} - \\eta \\, \\frac{\\partial L}{\\partial W_{o}} = W_{o} - \\eta \\, (\\hat{y} - y) \\, h^{\\top}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149a61f",
   "metadata": {},
   "source": [
    "\n",
    "**Hidden Layer Weights (`W_h`, shape DÃ—V):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial W_{h}}}_{D \\times V} = \\left(\\underbrace{W_{o}^{\\top}}_{D \\times V} \\underbrace{(\\hat{y} - y)}_{V \\times 1}\\right) \\, \\underbrace{x^{\\top}}_{1 \\times V}.\n",
    "$$\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\n",
    "W_{h} \\leftarrow W_{h} - \\eta \\, \\frac{\\partial L}{\\partial W_{h}} = W_{h} - \\eta \\, \\left(W_{o}^{\\top} (\\hat{y} - y)\\right) \\, x^{\\top}.\n",
    "$$\n",
    "\n",
    "Because in our case the input $x$ is one-hot (with active index $i$), only column $i$ of `W_h` gets updated:\n",
    "\n",
    "$$\n",
    "W_{h}[:, i] \\leftarrow W_{h}[:, i] - \\eta \\, W_{o}^{\\top} (\\hat{y} - y).\n",
    "$$\n",
    "\n",
    "All other columns remain unchanged for that training example.\n",
    "\n",
    "{{< video videos/BiGramTraining.mp4 >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a954b",
   "metadata": {},
   "source": [
    "## The Skip-Gram Model\n",
    "\n",
    "In the bi-gram model, we have seen how we can embed words in a dense (fixed-size) vector space by predicting the next word based on the current word and training a neural network with a hidden layer to do so.\n",
    "\n",
    "We now extend this idea to try to produce embeddings that capture more context by predicting surrounding words based on the current word. In effect, instead of generating a single output word, we will generate multiple output words (the context) based on a single input word (the center word). This is known as the skip-gram model.\n",
    "\n",
    "Let's take a break from Alice in Wonderland and consider the following sentence (from The Picture of Dorian Gray by Oscar Wilde):\n",
    "\n",
    "**They are simply cheques that men draw on a bank where they have no account.**\n",
    "\n",
    "1. _**They** are simply_ cheques that men draw on a bank where they have no account.\n",
    "    Center: They\n",
    "    Context: are, simply\n",
    "2. _They **are** simply cheques_ that men draw on a bank where they have no account.\n",
    "    Center: are\n",
    "    Context: They, simply, cheques\n",
    "4. _They are **simply** cheques that__ men draw on a bank where they have no account.\n",
    "    Center: simply\n",
    "    Context: They, are, cheques, that\n",
    "5. They _are simply **cheques** that men_ draw on a bank where they have no account.\n",
    "    Center: cheques\n",
    "    Context: are, simply, that, men\n",
    "6. They are __simply cheques **that** men draw_ on a bank where they have no account.\n",
    "7. They are simply _cheques that **men** draw on__ a bank where they have no account.\n",
    "8. They are simply cheques _that men **draw** on a_ bank where they have no account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56d225a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Center word: 'they' (index 0)\n",
      "  Context word: 'are' (index 1)\n",
      "  Context word: 'simply' (index 2)\n",
      "\n",
      "Center word: 'are' (index 1)\n",
      "  Context word: 'they' (index 0)\n",
      "  Context word: 'simply' (index 2)\n",
      "  Context word: 'cheques' (index 3)\n",
      "\n",
      "Center word: 'simply' (index 2)\n",
      "  Context word: 'they' (index 0)\n",
      "  Context word: 'are' (index 1)\n",
      "  Context word: 'cheques' (index 3)\n",
      "  Context word: 'that' (index 4)\n",
      "\n",
      "Center word: 'cheques' (index 3)\n",
      "  Context word: 'are' (index 1)\n",
      "  Context word: 'simply' (index 2)\n",
      "  Context word: 'that' (index 4)\n",
      "  Context word: 'men' (index 5)\n",
      "\n",
      "Center word: 'that' (index 4)\n",
      "  Context word: 'simply' (index 2)\n",
      "  Context word: 'cheques' (index 3)\n",
      "  Context word: 'men' (index 5)\n",
      "  Context word: 'draw' (index 6)\n",
      "\n",
      "Center word: 'men' (index 5)\n",
      "  Context word: 'cheques' (index 3)\n",
      "  Context word: 'that' (index 4)\n",
      "  Context word: 'draw' (index 6)\n",
      "  Context word: 'on' (index 7)\n",
      "\n",
      "Center word: 'draw' (index 6)\n",
      "  Context word: 'that' (index 4)\n",
      "  Context word: 'men' (index 5)\n",
      "  Context word: 'on' (index 7)\n",
      "  Context word: 'a' (index 8)\n",
      "\n",
      "Center word: 'on' (index 7)\n",
      "  Context word: 'men' (index 5)\n",
      "  Context word: 'draw' (index 6)\n",
      "  Context word: 'a' (index 8)\n",
      "  Context word: 'bank' (index 9)\n",
      "\n",
      "Center word: 'a' (index 8)\n",
      "  Context word: 'draw' (index 6)\n",
      "  Context word: 'on' (index 7)\n",
      "  Context word: 'bank' (index 9)\n",
      "  Context word: 'where' (index 10)\n",
      "\n",
      "Center word: 'bank' (index 9)\n",
      "  Context word: 'on' (index 7)\n",
      "  Context word: 'a' (index 8)\n",
      "  Context word: 'where' (index 10)\n",
      "  Context word: 'they' (index 11)\n",
      "\n",
      "Center word: 'where' (index 10)\n",
      "  Context word: 'a' (index 8)\n",
      "  Context word: 'bank' (index 9)\n",
      "  Context word: 'they' (index 11)\n",
      "  Context word: 'have' (index 12)\n",
      "\n",
      "Center word: 'they' (index 11)\n",
      "  Context word: 'bank' (index 9)\n",
      "  Context word: 'where' (index 10)\n",
      "  Context word: 'have' (index 12)\n",
      "  Context word: 'no' (index 13)\n",
      "\n",
      "Center word: 'have' (index 12)\n",
      "  Context word: 'where' (index 10)\n",
      "  Context word: 'they' (index 11)\n",
      "  Context word: 'no' (index 13)\n",
      "  Context word: 'account' (index 14)\n",
      "\n",
      "Center word: 'no' (index 13)\n",
      "  Context word: 'they' (index 11)\n",
      "  Context word: 'have' (index 12)\n",
      "  Context word: 'account' (index 14)\n",
      "\n",
      "Center word: 'account' (index 14)\n",
      "  Context word: 'have' (index 12)\n",
      "  Context word: 'no' (index 13)\n",
      "Tokens: ['they', 'are', 'simply', 'cheques', 'that', 'men', 'draw', 'on', 'a', 'bank', 'where', 'they', 'have', 'no', 'account']\n",
      "Skip-gram samples (center, context):\n",
      "('they', 'are')\n",
      "('they', 'simply')\n",
      "('are', 'they')\n",
      "('are', 'simply')\n",
      "('are', 'cheques')\n",
      "('simply', 'they')\n",
      "('simply', 'are')\n",
      "('simply', 'cheques')\n",
      "('simply', 'that')\n",
      "('cheques', 'are')\n",
      "('cheques', 'simply')\n",
      "('cheques', 'that')\n",
      "('cheques', 'men')\n",
      "('that', 'simply')\n",
      "('that', 'cheques')\n",
      "('that', 'men')\n",
      "('that', 'draw')\n",
      "('men', 'cheques')\n",
      "('men', 'that')\n",
      "('men', 'draw')\n",
      "('men', 'on')\n",
      "('draw', 'that')\n",
      "('draw', 'men')\n",
      "('draw', 'on')\n",
      "('draw', 'a')\n",
      "('on', 'men')\n",
      "('on', 'draw')\n",
      "('on', 'a')\n",
      "('on', 'bank')\n",
      "('a', 'draw')\n",
      "('a', 'on')\n",
      "('a', 'bank')\n",
      "('a', 'where')\n",
      "('bank', 'on')\n",
      "('bank', 'a')\n",
      "('bank', 'where')\n",
      "('bank', 'they')\n",
      "('where', 'a')\n",
      "('where', 'bank')\n",
      "('where', 'they')\n",
      "('where', 'have')\n",
      "('they', 'bank')\n",
      "('they', 'where')\n",
      "('they', 'have')\n",
      "('they', 'no')\n",
      "('have', 'where')\n",
      "('have', 'they')\n",
      "('have', 'no')\n",
      "('have', 'account')\n",
      "('no', 'they')\n",
      "('no', 'have')\n",
      "('no', 'account')\n",
      "('account', 'have')\n",
      "('account', 'no')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"They are simply cheques that men draw on a bank where they have no account\".lower()\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "window_size = 2\n",
    "skipgram_samples = []\n",
    "\n",
    "for idx, center_word in enumerate(tokens):\n",
    "    print(f\"\\nCenter word: '{center_word}' (index {idx})\")\n",
    "    \n",
    "    for offset in range(-window_size, window_size + 1):\n",
    "        context_idx = idx + offset\n",
    "        \n",
    "        if offset == 0 or context_idx < 0 or context_idx >= len(tokens):\n",
    "            continue\n",
    "        context_word = tokens[context_idx]\n",
    "\n",
    "        print(f\"  Context word: '{context_word}' (index {context_idx})\")\n",
    "        skipgram_samples.append((center_word, context_word))\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Skip-gram samples (center, context):\")\n",
    "for sample in skipgram_samples:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643c3d4",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "The skip-gram model until now only differs from the bi-gram model in the way we generate the training samples. However, it turns out that outputting a prediction for the whole vocabulary at each step is computationally expensive, especially for large vocabularies. Computing the softmax over a large vector is computationally intensive and requires updating a large number of parameters during training. To address this, the authors of Word2Vec introduced a technique called negative sampling.\n",
    "\n",
    "Negative sampling means that instead of predicting the entire vocabulary, the model should only predict a small number of \"negative\" samples (words that are not in the context) along with the actual context words. This significantly reduces the computational load and speeds up training.\n",
    "\n",
    "Let's see how this looks like in our example sentence:\n",
    "\n",
    "| Center word  | Context word | Sample type | $y_i$ |\n",
    "|---------|---------|-------------|-------|\n",
    "| cheques | are     | positive    | 1     |\n",
    "| cheques | simply  | positive    | 1     |\n",
    "| cheques | that    | positive    | 1     |\n",
    "| cheques | men     | positive    | 1     |\n",
    "| cheques | happy    | negative    | 0     |\n",
    "| cheques | people    | negative    | 0     |\n",
    "| cheques | break      | negative    | 0     |\n",
    "| cheques | plan    | negative    | 0     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646bd01",
   "metadata": {},
   "source": [
    "The model objective of the model now is to minimize the loss for predicting the positive samples as 1 and the negative samples as 0 (using binary cross-entropy loss) instead of predicting the entire vocabulary distribution.\n",
    "\n",
    "The predicted probability for a context word $w_c$ given a center word $w_i$ is computed using the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma (w^{o}_{c} \\cdot w_{i}) = \\frac{1}{1 + \\exp(-w^{o}_{c} \\cdot w_{i})}\n",
    "$$\n",
    "\n",
    "and now $\\hat{y}$ is a single number between 0 and 1 representing the probability that $w_c$ is a context word of $w_i$, not an entire vector of probabilities over the vocabulary. \n",
    "\n",
    "To avoid confusion, let's write the binary cross-entropy loss for each sample $(w_i, w_c)$:\n",
    "\n",
    "1. Cheques - are (positive sample): \n",
    "   $$\n",
    "   L_{\\text{cheques-are}} = - y_{\\text{cheques-are}}\\log(\\hat{y}_{\\text{cheques-are}}) + (1 - y_{\\text{cheques-are}})\\log(1 - \\hat{y}_{\\text{cheques-are}}) = - \\log(\\hat{y}_{\\text{cheques-are}})\n",
    "    $$\n",
    "\n",
    "   Because the outcome indicator $y_{\\text{cheques-are}}$ is 1 for positive samples this reduces to\n",
    "\n",
    "    $$\n",
    "    L_{\\text{cheques-are}} = - \\log(\\hat{y}_{\\text{cheques-are}})\n",
    "    $$\n",
    "\n",
    "2. Cheques - happy (negative sample):\n",
    "    $$\n",
    "    L_{\\text{cheques-happy}} = - y_{\\text{cheques-happy}}\\log(\\hat{y}_{\\text{cheques-happy}}) + (1 - y_{\\text{cheques-happy}})\\log(1 - \\hat{y}_{\\text{cheques-happy}})\n",
    "    $$\n",
    "\n",
    "    Here the outcome indicator $y_{\\text{cheques-happy}}$ is 0 for negative samples, so this reduces to\n",
    "\n",
    "    $$\n",
    "    L_{\\text{cheques-happy}} = - \\log(1 - \\hat{y}_{\\text{cheques-happy}})\n",
    "    $$\n",
    "\n",
    "\n",
    "So for any center word $w_i$ and context/negative sample word $w_c$, the loss can be summarized as:\n",
    "\n",
    "The training for the center word \"cheques\" with the context words as in the table above would involve minimizing the following loss:\n",
    "\n",
    "$$\n",
    "L = - \\left( \\sum_{(w_i, w_c) \\in D^{+}} \\log(\\hat{y}) + \\sum_{(w_i, w_c) \\in D^{-}} \\log(1 - \\hat{y}) \\right)\n",
    "$$\n",
    "\n",
    "Where $D^{+}$ is the set of positive samples and $D^{-}$ is the set of negative samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ef1a4",
   "metadata": {},
   "source": [
    "{{< video videos/SkipGramTraining.mp4 >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1112222",
   "metadata": {},
   "source": [
    "## Choosing Negative Samples\n",
    "\n",
    "A natural question is how to choose the negative examples. The authors of the skip-gram model suggest using the uni-gram distribution raised to the 3/4 power in order sample frequent words less often.\n",
    "\n",
    "$$\n",
    "p(\"the\") = \\frac{\\text{frequency of \"the\"}}{\\text{total number of words}}\n",
    "$$\n",
    "\n",
    "The uni-gram distribution raised to the 3/4 power is:\n",
    "\n",
    "$$\n",
    "p^1(\"the\") = \\frac{p(\\text{\"the\"})^{3/4}}{\n",
    "\\sum_{w \\in V} p(w)^{3/4}}\n",
    "$$\n",
    "\n",
    "where the sum in the denominator is the sum of the frequency of each word in the corpus raised to the 3/4 power so that $p^1$ is a probability distribution over the vocabulary. The 3/4 power is used to reduce the probability of sampling very frequent words and is a hyperparameter of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db653ad4",
   "metadata": {},
   "source": [
    "## Subsampling Frequent Words\n",
    "\n",
    "A pattern occurring in most human language texts is that some words are much more frequent than others. There is an empirical finding known as Zipf's law that states that the frequency of a word is inversely proportional to its rank in the frequency table. For example, the most frequent word will occur twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "$$\n",
    "\\text{word frequency} \\propto \\frac{1}{\\text{word rank}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "282faf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "gutenberg.fileids()\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b7c570d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1616),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 631),\n",
       " ('she', 545),\n",
       " ('i', 543),\n",
       " ('it', 540),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 397),\n",
       " ('was', 367),\n",
       " ('in', 359),\n",
       " ('you', 359),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " ('at', 209),\n",
       " ('on', 191),\n",
       " ('had', 185),\n",
       " ('with', 179)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "words = word_tokenize(alice)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "words_freq_dist = FreqDist(words)\n",
    "words_freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93b9a4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get frequencies sorted by rank\n",
    "freqs = np.array(sorted(words_freq_dist.values(), reverse=True))\n",
    "ranks = np.arange(1, len(freqs) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ranks, freqs, label=\"Word frequencies\")\n",
    "plt.plot(ranks, freqs[0] / ranks, label=\"Zipf's law (1/rank)\", linestyle='--')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Word Frequency Distribution vs Zipf's Law\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b37fcb",
   "metadata": {},
   "source": [
    "Because some words are used very often (e.g., the), they provide little information about the context. Furthermore, the training will spend a lot of time learning the word vectors for these words.\n",
    "\n",
    "A solution is to drop some of the frequent words from the training data. The authors of the skip-gram model suggest dropping words with a probability of:\n",
    "\n",
    "$$\n",
    "P_{drop}(w) = 1 - \\sqrt{\\frac{\\text{threshold}}{p^1(w)}}\n",
    "$$\n",
    "\n",
    "with a threshold of $t = 10^{-5}$ and $p^1(w)$ is modified uni-gram distribution.\n",
    "\n",
    "Consider our running example:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "and suppose that we drop the words \"get\" and \"of\" of the words in the sentence. The sentence will become:\n",
    "\n",
    "```\n",
    "Alice was beginning to very **tired** sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "What is the effect of this technique on the context window size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9631d",
   "metadata": {},
   "source": [
    "## Gradient Descent Updates with Negative Sampling\n",
    "\n",
    "With the model\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} & = \\sigma (w^{o}_{c} \\cdot w_{i}) \\\\\n",
    "L & = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that as only the embeddings of the center word $w_i$ and the context/negative sample word $w_c$ are involved in the computation of $\\hat{y}$, only these embeddings will be updated during training because the loss $L$ does not depend on any other embeddings.\n",
    "\n",
    "For the full derivation see the technical appendix. Here we present only the final gradient descent update equations.\n",
    "\n",
    "**Output Layer Embedding (`w^o_c`, shape DÃ—1):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial w^{o}_{c}}}_{D \\times 1} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial w^{o}_{c}} = \\underbrace{(\\hat{y} - y)}_{\\text{scalar}} \\, \\underbrace{w_{i}}_{D \\times 1}\n",
    "$$\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\n",
    "w^{o}_{c} \\leftarrow w^{o}_{c} - \\eta \\, \\frac{\\partial L}{\\partial w^{o}_{c}} = w^{o}_{c} - \\eta \\, (\\hat{y} - y) \\, w_{i}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Hidden Layer Embedding (`w_i`, shape DÃ—1):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial w_{i}}}_{D \\times 1} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial w_{i}} = \\underbrace{(\\hat{y} - y)}_{\\text{scalar}} \\, \\underbrace{w^{o}_{c}}_{D \\times 1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e4883",
   "metadata": {},
   "source": [
    "## Implementation in Numpy\n",
    "\n",
    "Next we want to implement the skip-gram model with negative sampling in Numpy. We need several steps and for ease of understanding, we will break them up into separate code cells.\n",
    "\n",
    "1. Prepare the data\n",
    "   - We need to split the text into sentences, then into words, and build a vocabulary.\n",
    "   - We need to know the size of the vocabulary $V$ and the embedding dimension $D$ in order to set up the weight matrices.\n",
    "   - We need to know the index of each word in the vocabulary to be able to access its embedding vector.\n",
    "   - We need to generate the training samples (center word, context word) pairs along with negative samples for each center word.\n",
    "   - We need to compute the unigram distribution raised to the 3/4 power for negative sampling.\n",
    "\n",
    "2. Initialize the embeddings\n",
    "   - We need to initialize two embedding matrices: one for the input (center words) and one for the output (context/negative sample words).\n",
    "   - The input embedding matrix will have shape (D, V) and the output embedding matrix will have shape (V, D).\n",
    "\n",
    "3. The actual training loop\n",
    "   - For each training sample (center word, context word, positive/negative label):\n",
    "     - Compute the predicted probability $\\hat{y}$ using the current embeddings.\n",
    "     - Compute the loss using binary cross-entropy.\n",
    "     - Compute the gradients with respect to the embeddings.\n",
    "     - Update the embeddings using gradient descent.\n",
    "  \n",
    "Below is a simplified implementation of the skip-gram model with negative sampling in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cccf83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf1fde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def train_skipgram(tokens, word2idx, idx2word, word_freq, embedding_dim=10, \n",
    "                   learning_rate=0.01, window_size=2, num_negative_samples=5, \n",
    "                   epochs=1, subsampling_threshold=1e-3):\n",
    "    \"\"\"\n",
    "    Train Skip-Gram model with negative sampling.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        word2idx: Word to index mapping\n",
    "        idx2word: Index to word mapping\n",
    "        word_freq: Word frequencies dictionary\n",
    "        embedding_dim: Dimension of embeddings\n",
    "        learning_rate: Learning rate\n",
    "        window_size: Context window size\n",
    "        num_negative_samples: Number of negative samples\n",
    "        epochs: Number of training epochs\n",
    "        subsampling_threshold: Threshold for subsampling\n",
    "        \n",
    "    Returns:\n",
    "        W_in: Trained input embeddings\n",
    "    \"\"\"\n",
    "    vocab_size = len(word2idx)\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    W_in, W_out = initialize_embeddings(vocab_size, embedding_dim)\n",
    "    \n",
    "    # Compute unigram distribution for negative sampling\n",
    "    unigram_dist = compute_unigram_distribution(word_freq, vocab_size)\n",
    "    \n",
    "    # Compute subsampling probabilities\n",
    "    keep_probs = compute_subsampling_probs(tokens, word2idx, word_freq, subsampling_threshold)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for idx, word in enumerate(tokens):\n",
    "            # Apply subsampling\n",
    "            if np.random.random() > keep_probs.get(word, 1.0):\n",
    "                continue\n",
    "            \n",
    "            if word not in word2idx:\n",
    "                continue\n",
    "            \n",
    "            center_idx = word2idx[word]\n",
    "            \n",
    "            # Generate context words within window\n",
    "            for offset in range(-window_size, window_size + 1):\n",
    "                if offset == 0:\n",
    "                    continue\n",
    "                \n",
    "                context_idx_pos = idx + offset\n",
    "                if context_idx_pos < 0 or context_idx_pos >= len(tokens):\n",
    "                    continue\n",
    "                \n",
    "                context_word = tokens[context_idx_pos]\n",
    "                if context_word not in word2idx:\n",
    "                    continue\n",
    "                \n",
    "                context_idx = word2idx[context_word]\n",
    "                \n",
    "                # Train on this pair\n",
    "                train_pair(center_idx, context_idx, W_in, W_out, learning_rate,\n",
    "                          num_negative_samples, unigram_dist, vocab_size)\n",
    "                total_pairs += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Processed {total_pairs} training pairs\")\n",
    "    \n",
    "    return W_in, W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6323746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 34\n",
      "Sample tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'the', 'dog', 'was', 'lazy', 'and', 'slept', 'all', 'day', '.', 'the']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens[:\u001b[32m20\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Build vocabulary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m word2idx, idx2word, word_freq = \u001b[43mbuild_vocabulary\u001b[49m(tokens, min_count=\u001b[32m1\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mVocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(word2idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMost common 10 words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(word_freq.items(),\u001b[38;5;250m \u001b[39mkey=\u001b[38;5;28;01mlambda\u001b[39;00m\u001b[38;5;250m \u001b[39mx:\u001b[38;5;250m \u001b[39mx[\u001b[32m1\u001b[39m],\u001b[38;5;250m \u001b[39mreverse=\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'build_vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: Train Skip-Gram model\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "The dog was lazy and slept all day.\n",
    "The fox was quick and clever.\n",
    "A brown dog ran through the woods.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and prepare data\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Sample tokens: {tokens[:20]}\")\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, word_freq = build_vocabulary(tokens, min_count=1)\n",
    "print(f\"\\nVocabulary size: {len(word2idx)}\")\n",
    "print(f\"Most common 10 words: {sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Skip-Gram Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "W_in, W_out = train_skipgram(\n",
    "    tokens=tokens,\n",
    "    word2idx=word2idx,\n",
    "    idx2word=idx2word,\n",
    "    word_freq=word_freq,\n",
    "    embedding_dim=5,\n",
    "    learning_rate=0.01,\n",
    "    window_size=2,\n",
    "    num_negative_samples=5,\n",
    "    epochs=10,\n",
    "    subsampling_threshold=1e-3\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Embedding dimensions: {W_in.shape}\")  # (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161d6c1",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862298a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversatio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Read the full text of \"Alice in Wonderland\" from the Gutenberg corpus\n",
    "alice_text = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "print(alice_text[:300])  # Print the first 1000 characters as a preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "alice\n",
      "adventures\n",
      "in\n",
      "wonderland\n",
      "by\n",
      "lewis\n",
      "carroll\n",
      "chapter\n",
      "i\n",
      "####################\n",
      "down\n",
      "the\n",
      "alice\n",
      "was\n",
      "beginning\n",
      "to\n",
      "get\n",
      "very\n",
      "tired\n",
      "of\n",
      "sitting\n",
      "by\n",
      "her\n",
      "sister\n",
      "on\n",
      "the\n",
      "bank\n",
      "and\n",
      "of\n",
      "having\n",
      "nothing\n",
      "to\n",
      "do\n",
      "once\n",
      "or\n",
      "twice\n",
      "she\n",
      "had\n",
      "peeped\n",
      "into\n",
      "the\n",
      "book\n",
      "her\n",
      "sister\n",
      "was\n",
      "reading\n",
      "but\n",
      "it\n",
      "had\n",
      "no\n",
      "pictures\n",
      "or\n",
      "conversations\n",
      "in\n",
      "it\n",
      "what\n",
      "is\n",
      "the\n",
      "use\n",
      "of\n",
      "a\n",
      "book\n",
      "thought\n",
      "alice\n",
      "pictures\n",
      "or\n",
      "conversation\n"
     ]
    }
   ],
   "source": [
    "# Get a list of sentences and their tokens\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(alice_text)\n",
    "\n",
    "# A list (sentences) of lists (tokens in a single sentence) of tokens\n",
    "tokenized_sentences = [\n",
    "    [token for token in word_tokenize(sentence.lower()) if token.isalpha()] for sentence in sentences\n",
    "    ]\n",
    "\n",
    "# Print the first few sentences as preview\n",
    "\n",
    "for sent_tokens in tokenized_sentences[:2]:\n",
    "    print(\"#\" * 20)\n",
    "    for token in sent_tokens:\n",
    "        print(token)\n",
    "    \n",
    "all_tokens = [token for sentence in tokenized_sentences for token in sentence if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 25501\n",
      "Vocabulary size: 2470\n"
     ]
    }
   ],
   "source": [
    "# Count unique extracted tokens, this is the vocabulary size\n",
    "\n",
    "V = len(set(all_tokens))\n",
    "\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Vocabulary size: {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of alice: 0\n",
      "Index of wonderland: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from word to index and vice versa\n",
    "\n",
    "word2idx = {word: idx for idx, (word, _) in enumerate(word_freq_dist.items())}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(\"Index of alice:\", word2idx.get(\"alice\", \"Not found\"))\n",
    "print(\"Index of wonderland:\", word2idx.get(\"wonderland\", \"Not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word at index of alice: alice\n",
      "Word at index of wonderland: wonderland\n"
     ]
    }
   ],
   "source": [
    "# Check that the reverse mapping works\n",
    "\n",
    "print(\"Word at index of alice:\", idx2word.get(0, \"Not found\"))\n",
    "print(\"Word at index of wonderland:\", idx2word.get(3, \"Not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a483de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1616),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 631),\n",
       " ('she', 545),\n",
       " ('i', 543),\n",
       " ('it', 540),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 397),\n",
       " ('was', 367),\n",
       " ('in', 359),\n",
       " ('you', 359),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " ('at', 209),\n",
       " ('on', 191),\n",
       " ('had', 185),\n",
       " ('with', 179)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After that, compute the word frequencies\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "word_freq_dist = FreqDist(all_tokens)\n",
    "word_freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e66810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "frequency",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8d602ad7-9162-4379-8c3b-71d8d61edf85",
       "rows": [
        [
         "10",
         "the",
         "1616"
        ],
        [
         "23",
         "and",
         "810"
        ],
        [
         "13",
         "to",
         "720"
        ],
        [
         "44",
         "a",
         "631"
        ],
        [
         "30",
         "she",
         "545"
        ],
        [
         "8",
         "i",
         "543"
        ],
        [
         "37",
         "it",
         "540"
        ],
        [
         "17",
         "of",
         "499"
        ],
        [
         "221",
         "said",
         "462"
        ],
        [
         "0",
         "alice",
         "397"
        ],
        [
         "11",
         "was",
         "367"
        ],
        [
         "2",
         "in",
         "359"
        ],
        [
         "231",
         "you",
         "359"
        ],
        [
         "83",
         "that",
         "284"
        ],
        [
         "51",
         "as",
         "256"
        ],
        [
         "19",
         "her",
         "248"
        ],
        [
         "103",
         "at",
         "209"
        ],
        [
         "21",
         "on",
         "191"
        ],
        [
         "31",
         "had",
         "185"
        ],
        [
         "76",
         "with",
         "179"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>a</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>she</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>it</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>of</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>said</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alice</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>was</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>you</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>that</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>as</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>her</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>at</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>on</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>had</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>with</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  frequency\n",
       "10     the       1616\n",
       "23     and        810\n",
       "13      to        720\n",
       "44       a        631\n",
       "30     she        545\n",
       "8        i        543\n",
       "37      it        540\n",
       "17      of        499\n",
       "221   said        462\n",
       "0    alice        397\n",
       "11     was        367\n",
       "2       in        359\n",
       "231    you        359\n",
       "83    that        284\n",
       "51      as        256\n",
       "19     her        248\n",
       "103     at        209\n",
       "21      on        191\n",
       "31     had        185\n",
       "76    with        179"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put it in a DataFrame for better handling\n",
    "import pandas as pd\n",
    "\n",
    "word_freq_df = pd.DataFrame(word_freq_dist.items(), columns=['word', 'frequency']).sort_values(by='frequency', ascending=False)\n",
    "word_freq_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9880742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "word_idx",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "frequency",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "p",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p_drop",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "dc69f112-a030-4684-98cc-daf58ab02a24",
       "rows": [
        [
         "0",
         "alice",
         "397",
         "0.015568016940512137",
         "0.008780144363804568",
         "0.8932791755433553"
        ],
        [
         "1",
         "adventures",
         "7",
         "0.0002744990392533626",
         "0.00042484675614128304",
         "0.5148412742821855"
        ],
        [
         "2",
         "in",
         "359",
         "0.014077879298851025",
         "0.008141972159526004",
         "0.889175649820562"
        ],
        [
         "3",
         "wonderland",
         "3",
         "0.00011764244539429826",
         "0.00022503481753358907",
         "0.33338490887887595"
        ],
        [
         "4",
         "by",
         "58",
         "0.002274420610956433",
         "0.0020748168759895788",
         "0.780461792405185"
        ],
        [
         "5",
         "lewis",
         "1",
         "3.921414846476609e-05",
         "9.872082512182082e-05",
         "-0.006457896704570976"
        ],
        [
         "6",
         "carroll",
         "1",
         "3.921414846476609e-05",
         "9.872082512182082e-05",
         "-0.006457896704570976"
        ],
        [
         "7",
         "chapter",
         "12",
         "0.00047056978157719305",
         "0.0006364945819243128",
         "0.6036282953352861"
        ],
        [
         "8",
         "i",
         "543",
         "0.021293282616367985",
         "0.011104746497368053",
         "0.9051044875039918"
        ],
        [
         "9",
         "down",
         "102",
         "0.003999843143406141",
         "0.003168537893981524",
         "0.8223478174915251"
        ],
        [
         "10",
         "the",
         "1616",
         "0.063370063919062",
         "0.025161689219139274",
         "0.9369579825232787"
        ],
        [
         "11",
         "was",
         "367",
         "0.014391592486569155",
         "0.008277674118587557",
         "0.8900878159883198"
        ],
        [
         "12",
         "beginning",
         "14",
         "0.0005489980785067252",
         "0.0007145042285427478",
         "0.6258914805784324"
        ],
        [
         "13",
         "to",
         "720",
         "0.028234186894631582",
         "0.013721707231657788",
         "0.9146318390815565"
        ],
        [
         "14",
         "get",
         "44",
         "0.001725422532449708",
         "0.0016865463612781664",
         "0.7564989389290251"
        ],
        [
         "15",
         "very",
         "139",
         "0.005450766636602486",
         "0.0039964085255872125",
         "0.8418150864102625"
        ],
        [
         "16",
         "tired",
         "7",
         "0.0002744990392533626",
         "0.00042484675614128304",
         "0.5148412742821855"
        ],
        [
         "17",
         "of",
         "499",
         "0.019567860083918278",
         "0.010422794756470597",
         "0.9020492181094238"
        ],
        [
         "18",
         "sitting",
         "10",
         "0.00039214148464766086",
         "0.0005551479962288943",
         "0.5755802241115973"
        ],
        [
         "19",
         "her",
         "248",
         "0.00972510881926199",
         "0.006169465460832696",
         "0.8726859800051288"
        ],
        [
         "20",
         "sister",
         "9",
         "0.0003529273361828948",
         "0.000512968454628347",
         "0.5584756390054302"
        ],
        [
         "21",
         "on",
         "191",
         "0.007489902356770323",
         "0.005072053222009091",
         "0.8595867476217076"
        ],
        [
         "22",
         "bank",
         "3",
         "0.00011764244539429826",
         "0.00022503481753358907",
         "0.33338490887887595"
        ],
        [
         "23",
         "and",
         "810",
         "0.031763460256460534",
         "0.014988995898180146",
         "0.9183203760471433"
        ],
        [
         "24",
         "having",
         "10",
         "0.00039214148464766086",
         "0.0005551479962288943",
         "0.5755802241115973"
        ],
        [
         "25",
         "nothing",
         "30",
         "0.0011764244539429827",
         "0.0012654637750580688",
         "0.7188907469414962"
        ],
        [
         "26",
         "do",
         "128",
         "0.0050194110034900595",
         "0.0037567842442818786",
         "0.8368481986828316"
        ],
        [
         "27",
         "once",
         "32",
         "0.0012548527508725149",
         "0.0013282238072932476",
         "0.7256124702604136"
        ],
        [
         "28",
         "or",
         "71",
         "0.002784204540998392",
         "0.0024146408196604055",
         "0.7964956340559531"
        ],
        [
         "29",
         "twice",
         "5",
         "0.00019607074232383043",
         "0.0003300929734974524",
         "0.4495956468518024"
        ],
        [
         "30",
         "she",
         "545",
         "0.021371710913297516",
         "0.011135408490862397",
         "0.9052352277165602"
        ],
        [
         "31",
         "had",
         "185",
         "0.007254617465981726",
         "0.004952079097787105",
         "0.857896029200728"
        ],
        [
         "32",
         "peeped",
         "3",
         "0.00011764244539429826",
         "0.00022503481753358907",
         "0.33338490887887595"
        ],
        [
         "33",
         "into",
         "67",
         "0.002627347947139328",
         "0.0023118778211408545",
         "0.792021921869867"
        ],
        [
         "34",
         "book",
         "7",
         "0.0002744990392533626",
         "0.00042484675614128304",
         "0.5148412742821855"
        ],
        [
         "35",
         "reading",
         "3",
         "0.00011764244539429826",
         "0.00022503481753358907",
         "0.33338490887887595"
        ],
        [
         "36",
         "but",
         "131",
         "0.005137053448884358",
         "0.003822629863704094",
         "0.8382594648102861"
        ],
        [
         "37",
         "it",
         "540",
         "0.021175640170973686",
         "0.011058700503318832",
         "0.9049071307179187"
        ],
        [
         "38",
         "no",
         "77",
         "0.0030194894317869887",
         "0.0025661191666267393",
         "0.8025934358238737"
        ],
        [
         "39",
         "pictures",
         "4",
         "0.00015685659385906436",
         "0.0002792246595518831",
         "0.4015565541441253"
        ],
        [
         "40",
         "conversations",
         "1",
         "3.921414846476609e-05",
         "9.872082512182082e-05",
         "-0.006457896704570976"
        ],
        [
         "41",
         "what",
         "103",
         "0.004039057291870907",
         "0.003191807531164713",
         "0.822996582158321"
        ],
        [
         "42",
         "is",
         "114",
         "0.004470412924983334",
         "0.003444192487348424",
         "0.8296052175220519"
        ],
        [
         "43",
         "use",
         "18",
         "0.0007058546723657896",
         "0.0008627066692704294",
         "0.6595381753964253"
        ],
        [
         "44",
         "a",
         "631",
         "0.0247441276812674",
         "0.012428845688531387",
         "0.9103016194923471"
        ],
        [
         "45",
         "thought",
         "74",
         "0.0029018469863926906",
         "0.0024907639215966462",
         "0.7996295300403728"
        ],
        [
         "46",
         "conversation",
         "10",
         "0.00039214148464766086",
         "0.0005551479962288943",
         "0.5755802241115973"
        ],
        [
         "47",
         "so",
         "144",
         "0.005646837378926317",
         "0.004103747637026776",
         "0.8438975651408412"
        ],
        [
         "48",
         "considering",
         "3",
         "0.00011764244539429826",
         "0.00022503481753358907",
         "0.33338490887887595"
        ],
        [
         "49",
         "own",
         "10",
         "0.00039214148464766086",
         "0.0005551479962288943",
         "0.5755802241115973"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2470
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>p</th>\n",
       "      <th>p1</th>\n",
       "      <th>p_drop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alice</td>\n",
       "      <td>397</td>\n",
       "      <td>0.015568</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.893279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventures</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.514841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>359</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>0.889176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.333385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>58</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.780462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>gather</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.006458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>sorrows</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.006458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>joys</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.006458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>remembering</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.006458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>happy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.006458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2470 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  frequency         p        p1    p_drop\n",
       "word_idx                                                      \n",
       "0               alice        397  0.015568  0.008780  0.893279\n",
       "1          adventures          7  0.000274  0.000425  0.514841\n",
       "2                  in        359  0.014078  0.008142  0.889176\n",
       "3          wonderland          3  0.000118  0.000225  0.333385\n",
       "4                  by         58  0.002274  0.002075  0.780462\n",
       "...               ...        ...       ...       ...       ...\n",
       "2465           gather          1  0.000039  0.000099 -0.006458\n",
       "2466          sorrows          1  0.000039  0.000099 -0.006458\n",
       "2467             joys          1  0.000039  0.000099 -0.006458\n",
       "2468      remembering          1  0.000039  0.000099 -0.006458\n",
       "2469            happy          1  0.000039  0.000099 -0.006458\n",
       "\n",
       "[2470 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the subsampling probabilities\n",
    "\n",
    "word_freq_df[\"p\"] = word_freq_df[\"frequency\"] / word_freq_df[\"frequency\"].sum()\n",
    "word_freq_df[\"p1\"] = word_freq_df[\"p\"]**(3/4)\n",
    "word_freq_df[\"p1\"] = word_freq_df[\"p1\"] / word_freq_df[\"p1\"].sum()\n",
    "\n",
    "word_freq_df[\"word_idx\"] = word_freq_df[\"word\"].map(word2idx)\n",
    "word_freq_df = word_freq_df.set_index(\"word_idx\").sort_index()\n",
    "\n",
    "drop_threshold = 1e-4\n",
    "\n",
    "word_freq_df[\"p_drop\"] = 1 - np.sqrt(drop_threshold / word_freq_df[\"p1\"])\n",
    "\n",
    "word_drop_p = word_freq_df[\"p_drop\"].to_dict()\n",
    "word_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8c749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8932791755433553"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_drop_p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c1986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[466, 1129, 1877, 17, 1817]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A helper function to select negative samples based on the unigram distribution\n",
    "\n",
    "def get_negative_samples(n):\n",
    "    neg_words = word_freq_df[\"word\"].sample(\n",
    "        n=n,\n",
    "        replace=True,\n",
    "        weights=word_freq_df[\"p1\"]\n",
    "    )\n",
    "\n",
    "    return [word2idx[word] for word in neg_words]\n",
    "\n",
    "\n",
    "get_negative_samples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9184059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training pairs from the full text. Each pair is (center_word_idx, context_word_idx, positive/negative label)\n",
    "\n",
    "training_pairs = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    sentence_length = len(sentence)\n",
    "    \n",
    "    # Skip too short sentences\n",
    "    if sentence_length < 2:\n",
    "        continue\n",
    "\n",
    "    # Apply subsampling\n",
    "    \n",
    "    training_tokens = []\n",
    "\n",
    "    for word in sentence:\n",
    "        random_value_between_0_1 = np.random.random()\n",
    "        drop_prob = word_drop_p.get(word2idx.get(word, -1), 0.0)\n",
    "        if random_value_between_0_1 > drop_prob:\n",
    "            training_tokens.append(word)\n",
    "    \n",
    "\n",
    "    sentence = [word for word in training_tokens]\n",
    "    sentence_length = len(training_tokens)\n",
    "        \n",
    "    for center_pos, center_word in enumerate(training_tokens):\n",
    "        center_idx = word2idx[center_word]\n",
    "        \n",
    "        # Define context window\n",
    "        window_start = max(0, center_pos - 2)\n",
    "        window_end = min(sentence_length, center_pos + 3)\n",
    "        \n",
    "        for context_pos in range(window_start, window_end):\n",
    "            if context_pos == center_pos:\n",
    "                continue\n",
    "            \n",
    "            context_word = sentence[context_pos]\n",
    "            if context_word not in word2idx:\n",
    "                continue\n",
    "            \n",
    "            context_idx = word2idx[context_word]\n",
    "            \n",
    "            # Positive sample\n",
    "            training_pairs.append((center_idx, context_idx, 1))\n",
    "            \n",
    "            # Negative samples (should fix)\n",
    "            negative_samples = get_negative_samples(4)\n",
    "            for neg_idx in negative_samples:\n",
    "                training_pairs.append((center_idx, neg_idx, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e72b7e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 115, 0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to write the training loop\n",
    "\n",
    "# First we initialize the embedding matrices\n",
    "\n",
    "Wh = np.random.randn(D, V) * 0.01\n",
    "Wo = np.random.randn(V, D) * 0.01\n",
    "\n",
    "# Then we say how many times we want to go through the full text\n",
    "epochs = 5\n",
    "\n",
    "# Also set the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training pairs each epoch so that the words are not always in the same order\n",
    "    np.random.shuffle(training_pairs)\n",
    "        \n",
    "    total_loss = 0\n",
    "    for center_idx, context_idx, y in training_pairs:\n",
    "        # Forward pass: compute predicted y_hat\n",
    "\n",
    "        w_i = Wh[:, center_idx]  # Hidden layer\n",
    "        z = Wo[context_idx, :].dot(w_i)  # Output layer\n",
    "\n",
    "        y_hat = 1 / (1 + np.exp(-z))  # Sigmoid: predicted probability\n",
    "        loss = - (y * np.log(y_hat + 1e-10) + (1 - y) * np.log(1 - y_hat + 1e-10))  # Loss \n",
    "\n",
    "        # Record the total loss for printing\n",
    "        total_loss += loss       \n",
    "\n",
    "        # Backward pass: update the weights\n",
    "\n",
    "        e = y_hat - y  # Error term\n",
    "        Wo[context_idx, :] -= learning_rate * e * w_i  # Update output weights\n",
    "        Wh[:, center_idx] -= learning_rate * e * Wo[context_idx, :]  # Update input weights\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(training_pairs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c87e2c",
   "metadata": {},
   "source": [
    "## Technical Appendix\n",
    "\n",
    "### Derivation of Gradient Descent Updates in the Bi-gram Model\n",
    "\n",
    "To derive the gradients for the gradient descent update equations we use the chain rule of calculus.\n",
    "\n",
    "For the model\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h & = W_{h} x \\\\\n",
    "z & = W_{o} h \\\\\n",
    "\\hat{y} & = \\text{softmax}(z) \\\\\n",
    "L & = -\\sum_{i=1}^{V} y_{i} \\log(\\hat{y}_{i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "we need to compute the partial derivatives of the loss $L$ with respect to the weights $W_{h}$ and $W_{o}$. Note that the loss function is a scalar (a real number) \n",
    "and that its gradient with respect to a matrix is also a matrix of the same shape as the original matrix. So the \n",
    "gradient of the loss with respect to $W_{o}$ is a matrix of shape (V, D) and the gradient of the loss with respect to $W_{h}$ is a matrix of shape (D, V).\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{o}} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w^{o}_{11}} & \\frac{\\partial L}{\\partial w^{o}_{12}} & \\dots & \\frac{\\partial L}{\\partial w^{o}_{1D}} \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{21}} & \\frac{\\partial L}{\\partial w^{o}_{22}} & \\dots & \\frac{\\partial L}{\\partial w^{o}_{2D}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{V1}} & \\frac{\\partial L}{\\partial w^{o}_{V2}} & \\dots & \\frac{\\partial L}{\\partial w^{o}_{VD}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial L}{\\partial W_{h}} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\dots & \\frac{\\partial L}{\\partial w_{1V}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\dots & \\frac{\\partial L}{\\partial w_{2V}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_{D1}} & \\frac{\\partial L}{\\partial w_{D2}} & \\dots & \\frac{\\partial L}{\\partial w_{DV}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When computing the gradients it helps to first derive the gradient of the loss with respect to the output layer $z$. It is a vector of shape (V, 1), so \n",
    "the gradient is also a vector of shape (V, 1). As all other derivatives involve this term, it is useful to derive it first.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial z_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial z_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial z_{V}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's start by deriving the loss with respect to a single element of the output layer $z_{k}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{k}} = \\sum_{i=1}^{V} \\frac{\\partial L}{\\partial \\hat{y}_{i}} \\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} \\quad \\text{(chain rule)}\n",
    "$$\n",
    "\n",
    "**Note on the summation:** We could write the loss as $L = \\sum_{i=1}^{V} L_i$ where $L_i = -y_{i} \\log(\\hat{y}_{i})$ is the contribution from category $i$. However, the sum in the chain rule above appears for a different reason: due to the softmax function, $z_k$ influences the total loss $L$ through *all* the predicted probabilities $\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_V$ (not just $\\hat{y}_k$). The summation accounts for all these coupled pathways through which changing $z_k$ affects $L$.\n",
    "\n",
    "The first derivative in the chain rule is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{i}} = - y_i \\frac{\\partial}{\\partial \\hat{y}_{i}} \\log(\\hat{y}_{i}) = - \\frac{y_i}{\\hat{y}_{i}}\n",
    "$$\n",
    "\n",
    "The second derivative is a little bit more confusing because of the softmax function.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} = \\frac{\\partial}{\\partial z_{k}} \\left( \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\right)\n",
    "$$\n",
    "\n",
    "We need to distinguish two cases: when $i = k$ and when $i \\neq k$ because the derivative will be different in these two cases (because $z_{k}$ appears in both the numerator and denominator of the softmax function).\n",
    "\n",
    "When $i = k$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{y}_{i}}{\\partial z_{i}} & = \\frac{\\partial}{\\partial z_{i}} \\left( \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\right) \\\\\n",
    "& = \\frac{\\exp(z_{i}) \\sum_{j=1}^{V} \\exp(z_{j}) - \\exp(z_{i}) \\exp(z_{i})}{\\left( \\sum_{j=1}^{V} \\exp(z_{j}) \\right)^{2}} \\\\\n",
    "& = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\left( 1 - \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\right) \\\\\n",
    "& = \\hat{y}_{i} (1 - \\hat{y}_{i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the other case, when $i \\neq k$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} & = \\frac{\\partial}{\\partial z_{k}} \\left( \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\right) \\\\\n",
    "& = - \\frac{\\exp(z_{i}) \\exp(z_{k})}{\\left( \\sum_{j=1}^{V} \\exp(z_{j}) \\right)^{2}} \\\\\n",
    "& = - \\frac{\\exp(z_{i})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\frac{\\exp(z_{k})}{\\sum_{j=1}^{V} \\exp(z_{j})} \\\\\n",
    "& = - \\hat{y}_{i} \\hat{y}_{k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To summarize:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} = \\begin{cases}\n",
    "\\hat{y}_{i} (1 - \\hat{y}_{i}) & \\text{if } i = k \\\\\n",
    "- \\hat{y}_{i} \\hat{y}_{k} & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad22e4b",
   "metadata": {},
   "source": [
    "Now let's use this result to compute the derivative of the loss with respect to $z_{k}$. What we do is to split the sum into two parts: one for the case when $i = k$ and another for the case when $i \\neq k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{k}} & = \\sum_{i=1}^{V} \\frac{\\partial L}{\\partial \\hat{y}_{i}} \\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} \\\\\n",
    "& = \\frac{\\partial L}{\\partial \\hat{y}_{k}} \\frac{\\partial \\hat{y}_{k}}{\\partial z_{k}} + \\sum_{i \\neq k} \\frac{\\partial L}{\\partial \\hat{y}_{i}} \\frac{\\partial \\hat{y}_{i}}{\\partial z_{k}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then we substitute with the definition of the loss function and the derivatives we just computed:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{k}} & = - \\frac{y_{k}}{\\cancel{\\hat{y}_{k}}} \\cancel{\\hat{y}_{k}} (1 - \\hat{y}_{k}) + \\sum_{i \\neq k} \\left(- \\frac{y_{i}}{\\cancel{\\hat{y}_{i}}}\\right) (- \\cancel{\\hat{y}_{i}} \\hat{y}_{k}) \\\\\n",
    "& = - y_{k} (1 - \\hat{y}_{k}) - \\sum_{i \\neq k} y_{i}\\hat{y}_{k} \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e316929",
   "metadata": {},
   "source": [
    "With this expression we can notice two things:\n",
    "\n",
    "1. $\\hat{y}_k$ does not depend on the index of summation $i$ so we can factor it out of the sum.\n",
    "2. The sum goes over all indices except $k$, so we can rewrite it as the full sum minus the term for index $k$ without changing its value. We do this\n",
    "   to simplify the expression because we know that the sum of all elements of $y$ is equal to 1 (because it is a probability distribution).\n",
    "\n",
    "These are the key considerations, what follows after that is just algebraic manipulation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{k}} & = - y_{k} (1 - \\hat{y}_{k}) - \\hat{y}_{k} \\sum_{i \\neq k} y_{i} \\\\\n",
    "& = - y_{k} (1 - \\hat{y}_{k}) - \\hat{y}_{k} \\left(\\sum_{i=1}^{V} y_{i} - y_{k}\\right) \\\\\n",
    "& = - y_{k} (1 - \\hat{y}_{k}) - \\hat{y}_{k} (1 - y_{k}) \\\\\n",
    "& = - y_{k} + y_{k} \\hat{y}_{k} - \\hat{y}_{k} + y_{k} \\hat{y}_{k} \\\\\n",
    "& = \\hat{y}_{k} - y_{k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which is simply the difference between predicted distribution and the actual probability (the error) for class (word) $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac9619",
   "metadata": {},
   "source": [
    "So the gradient of the loss with respect to the output layer pre-activation $z$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - y = \\begin{bmatrix}\n",
    "\\hat{y}_{1} - y_{1} \\\\\n",
    "\\hat{y}_{2} - y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_{V} - y_{V}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4397cb",
   "metadata": {},
   "source": [
    "Now that we have the intermediate step in the chain rule, let's develop it further to actually compute the gradients with respect to the output layer weights $W_{o}$ and then the hidden layer weights $W_{h}$.\n",
    "\n",
    "#### Output Layer Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w^{o}_{jk}} = \\sum_{i=1}^{V} \\frac{\\partial L}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial w^{o}_{jk}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f2251",
   "metadata": {},
   "source": [
    "We have already derived the first term in the chain rule, so now let's focus on the second term and for simplicity, let's choose the weight $w^{o}_{11}$. Furthermore, we'll copy the equation for $z_{i}$ here for convenience. We'll only change the index of the input word from $i$ to $l$ to avoid confusion with the index of summation.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_{1} \\\\\n",
    "z_{2} \\\\\n",
    "\\vdots \\\\\n",
    "z_{V}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w^{o}_{11} w_{1l} + w^{o}_{12} w_{2l} + \\dots + w^{o}_{1D} w_{Dl} \\\\\n",
    "w^{o}_{21} w_{1l} + w^{o}_{22} w_{2l} + \\dots + w^{o}_{2D} w_{Dl} \\\\\n",
    "\\vdots \\\\\n",
    "w^{o}_{V1} w_{1l} + w^{o}_{V2} w_{2l} + \\dots + w^{o}_{VD} w_{Dl}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "First row of $z$ ($i = 1$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{1}}{\\partial w^{o}_{11}} = \\frac{\\partial}{\\partial w^{o}_{11}} \\left( w^{o}_{11} w_{1l} + w^{o}_{12} w_{2l} + \\dots + w^{o}_{1D} w_{Dl}\\right) = w_{1l}\n",
    "$$\n",
    "\n",
    "Now we'll notice that the derivative of all other rows of $z$ with respect to $w^{o}_{11}$ will be zero because $w^{o}_{11}$ only appears in the first row of $z$. So we can summarize:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{i}}{\\partial w^{o}_{11}} = \\begin\n",
    "{cases}\n",
    "w_{1l} & \\text{if } i = 1 \\\\\n",
    "0 & \\text{if } i \\neq 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Continue doing this for $$w^{o}_{12}, w^{o}_{1 3}, \\dots, w^{o}_{1 D}$$ and write the final result of the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w^{o}_{11}} & = (\\hat{y}_1 - y_1) w_{1l} \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{12}} & = (\\hat{y}_1 - y_1) w_{2l} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{1D}} & = (\\hat{y}_1 - y_1) w_{Dl}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For $w^{o}_{21}, w^{o}_{22}, \\dots, w^{o}_{2D}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w^{o}_{21}} & = (\\hat{y}_2 - y_2) w_{1l} \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{22}} & = (\\hat{y}_2 - y_2) w_{2l} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w^{o}_{2D}} & = (\\hat{y}_2 - y_2) w_{Dl}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, combine all these columns into a matrix form:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{o}} = \\begin{bmatrix}\n",
    "(\\hat{y}_1 - y_1) w_{1l} & (\\hat{y}_1 - y_1) w_{2l} & \\dots & (\\hat{y}_1 - y_1) w_{Dl} \\\\\n",
    "(\\hat{y}_2 - y_2) w_{1l} & (\\hat{y}_2 - y_2) w_{2l} & \\dots & (\\hat{y}_2 - y_2) w_{Dl} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "(\\hat{y}_V - y_V) w_{1l} & (\\hat{y}_V - y_V) w_{2l} & \\dots & (\\hat{y}_V - y_V) w_{Dl}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What you have obtained in simply the outer product between the error vector $(\\hat{y} - y)$ of shape (V, 1) and the hidden layer vector $h$ of shape (1, D). In this special case of the input being one-hot encoded, the hidden layer $h$ is simply the column of $W_{h}$ corresponding to the index of the input word $l$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{o}} = (\\hat{y} - y) \\, h^{\\top} = (\\hat{y} - y) \\, W_{h}[:, l]^{\\top}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b84ea5",
   "metadata": {},
   "source": [
    "#### Hidden Layer Gradient\n",
    "\n",
    "For the hidden layer with weights we can proceed in a similar fashion. We start with the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}} = \\sum_{i=1}^{V} \\frac{\\partial L}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial w_{11}}\n",
    "$$\n",
    "\n",
    "For i = 1:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{1}}{\\partial w_{11}} = \\frac{\\partial}{\\partial w_{11}} \\left( w^{o}_{11} w_{11} + w^{o}_{12} w_{21} + \\dots + w^{o}_{1D} w_{D1} \\right) = w^{o}_{11}\n",
    "$$\n",
    "\n",
    "For i = 2:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{2}}{\\partial w_{11}} = \\frac{\\partial}{\\partial w_{11}} \\left( w^{o}_{21} w_{11} + w^{o}_{22} w_{21} + \\dots + w^{o}_{2D} w_{D1} \\right) = w^{o}_{21}\n",
    "$$\n",
    "\n",
    "Continuing this for all i, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{i}}{\\partial w_{11}} = w^{o}_{i1}\n",
    "$$\n",
    "\n",
    "Putting it all together in the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & = \\sum_{i=1}^{V} \\frac{\\partial L}{\\partial z_{i}} \\frac{\\partial z_{i}}{\\partial w_{11}} \\\\\n",
    "& = \\sum_{i=1}^{V} (\\hat{y}_i - y_i) w^{o}_{i1} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now this is nothing but the dot product between the output weights column vector $W_{o}[:, 1]$ and the error vector $(\\hat{y} - y)$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8a1a0",
   "metadata": {},
   "source": [
    "When you continue doing this you will notice that only the derivatives with respect to the weights in the column corresponding to the input word index $l$ will be non-zero because only these appear in the dot products in the rows of $z$.\n",
    "\n",
    "So for the entire vector of weights corresponding to the input word index $l$ we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{1l}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{2l}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_{Dl}}\n",
    "\\end{bmatrix} = W_{o}^{\\top} (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "As all other derivatives are zero, this means that gradient descent will only update the column of `W_h` corresponding to the input word index $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b1f79",
   "metadata": {},
   "source": [
    "## Derivation of Gradient Descent Updates (Negative Sampling)\n",
    "\n",
    "We repeat the model equations here for convenience:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} & = \\sigma (w^{o}_{c} \\cdot w_{i}) \\\\\n",
    "L & = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $w_i$ is a vector of shape (D, 1) representing the hidden layer embedding of the center word and $w^{o}_c$ is a vector of shape (D, 1) representing the output layer embedding of the context/negative sample word.\n",
    "\n",
    "As the loss function depends on the embeddings through the sigmoid function, let's first see what its derivative is. First, the sigmoid function was:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "We can use the quotient rule or rewrite it as $\\sigma(x) = (1 + \\exp(-x))^{-1}$ and use the chain rule. Either way, the result is the same.\n",
    "\n",
    "Using the quotient rule:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(x)}{dx} = \\frac{(1 + \\exp(-x)) \\cdot 0 - 1 \\cdot (-\\exp(-x))}{(1 + \\exp(-x))^{2}} = \\frac{\\exp(-x)}{(1 + \\exp(-x))^{2}}\n",
    "$$\n",
    "\n",
    "With a little bit of rearrangement we can express this derivative in terms of the sigmoid function itself:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\sigma(x)}{dx} & = \\frac{1}{1 + \\exp(-x)} \\cdot \\frac{\\exp(-x)}{1 + \\exp(-x)} \\\\\n",
    "& = \\sigma(x) \\left( 1 - \\frac{1}{1 + \\exp(-x)} \\right) \\\\\n",
    "& = \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38108572",
   "metadata": {},
   "source": [
    "\n",
    "Now we can derive the gradient of the loss with respect to the dot product of the embeddings which is simply a scalar, let's call it $a$:\n",
    "\n",
    "$$\n",
    "a = w^{o}_{c} \\cdot w_{i}\n",
    "$$\n",
    "\n",
    "The derivative of the loss with respect to $a$ is obtained using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial\n",
    "\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial a}\n",
    "$$\n",
    "\n",
    "So doing both at once and substituting the definitions of the loss function and the sigmoid derivative we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a} & = \\left( - y \\frac{\\partial}{\\partial \\hat{y}} \\log(\\hat{y}) - (1 - y) \\frac{\\partial}{\\partial \\hat{y}} \\log(1 - \\hat{y}) \\right) \\left( \\hat{y} (1 - \\hat{y}) \\right) \\\\\n",
    "& = \\left( - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\right) \\left( \\hat{y} (1 - \\hat{y}) \\right) \\\\\n",
    "& = - y (1 - \\hat{y}) + (1 - y) \\hat{y} \\\\\n",
    "& = \\hat{y} - y\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Again, the derivative of the loss with respect to the dot product of the embeddings is simply the difference between the predicted probability and the actual outcome indicator as in the full softmax case. This should not surprise you because both loss functions are forms of cross-entropy loss and the sigmoid function is a special case of the softmax function for two classes. Again, the difference is that the derivative is now a scalar instead of a vector.\n",
    "\n",
    "Now the derivatives with respect to the embeddings can be computed are easy because we are dealing with simple dot products and we have already computed the common part in the chain rule.\n",
    "\n",
    "**Output Layer Embedding (`w^o_c`, shape DÃ—1):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial w^{o}_{c}}}_{D \\times 1} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial w^{o}_{c}} = \\underbrace{(\\hat{y} - y)}_{\\text{scalar}} \\, \\underbrace{w_{i}}_{D \\times 1}\n",
    "$$\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\n",
    "w^{o}_{c} \\leftarrow w^{o}_{c} - \\eta \\, \\frac{\\partial L}{\\partial w^{o}_{c}} = w^{o}_{c} - \\eta \\, (\\hat{y} - y) \\, w_{i}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Hidden Layer Embedding (`w_i`, shape DÃ—1):**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial L}{\\partial w_{i}}}_{D \\times 1} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial w_{i}} = \\underbrace{(\\hat{y} - y)}_{\\text{scalar}} \\, \\underbrace{w^{o}_{c}}_{D \\times 1}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
