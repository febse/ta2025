{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxcbcOSTDjix"
   },
   "source": [
    "# Introduction {#introduction}\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/febse/ta2025/blob/main/01-Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Class Version In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction {#introduction}\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/febse/ta2025/blob/main/01-Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Class Version In Colab\"/></a>\n",
    "\n",
    "## Why Text Analytics?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Foundations (1950s-1960s)\n",
    "\n",
    "The field emerged from early computational linguistics and information retrieval. In 1957, **Noam Chomsky** revolutionized the field with his theory of generative grammar, providing a mathematical framework for language structure (Chomsky, 1957). The **Georgetown-IBM experiment** (1954) demonstrated the first machine translation, translating 60 Russian sentences into English—though the system was limited and the promise of \"solving\" translation in 3-5 years proved overly optimistic (Hutchins, 1997).\n",
    "\n",
    "**Memorable story**: The Georgetown experiment led to inflated expectations, followed by the \"AI winter\" when the 1966 ALPAC report revealed machine translation's limitations, cutting funding for decades.\n",
    "\n",
    "### Information Retrieval Era (1970s-1980s)\n",
    "\n",
    "Gerard Salton developed the **Vector Space Model** and **TF-IDF** weighting at Cornell, establishing foundational concepts still used today (Salton & McGill, 1983). The creation of **MEDLINE** and early search systems showed practical applications of text processing.\n",
    "\n",
    "### Statistical Revolution (1990s)\n",
    "\n",
    "The field shifted from rule-based to statistical approaches. **IBM's statistical machine translation** models (Brown et al., 1990) and the availability of large text corpora transformed the discipline. The **Penn Treebank** (Marcus et al., 1993) enabled data-driven NLP research.\n",
    "\n",
    "**Memorable story**: In 1997, IBM's translation team demonstrated that statistical methods could outperform decades of hand-crafted linguistic rules, fundamentally changing the field's direction.\n",
    "\n",
    "### Machine Learning Boom (2000s)\n",
    "\n",
    "**Topic modeling** emerged with Latent Dirichlet Allocation (Blei et al., 2003). **Sentiment analysis** became commercially valuable for brand monitoring and customer feedback. **Named Entity Recognition** and **information extraction** saw widespread adoption.\n",
    "\n",
    "**Memorable story**: The 2008 presidential election saw widespread use of sentiment analysis to track public opinion on social media—marking text analytics' entry into mainstream politics (O'Connor et al., 2010).\n",
    "\n",
    "### Deep Learning Era (2010s)\n",
    "\n",
    "**Word embeddings** (Word2Vec, GloVe) captured semantic relationships in vector space (Mikolov et al., 2013). **Recurrent Neural Networks** and **attention mechanisms** revolutionized sequence modeling. The **Transformer architecture** (Vaswani et al., 2017) became the foundation for modern NLP.\n",
    "\n",
    "**Memorable story**: Google's 2016 replacement of its entire translation system with neural networks overnight improved quality dramatically, translating decades of linguistic work into a single neural architecture (Wu et al., 2016).\n",
    "\n",
    "### The LLM Revolution (2018-Present)\n",
    "\n",
    "**BERT** (Devlin et al., 2018) introduced transfer learning to NLP. **GPT models** demonstrated few-shot and zero-shot learning capabilities. **ChatGPT** (2022) brought conversational AI to mainstream audiences, with 100 million users in two months—the fastest-growing application in history.\n",
    "\n",
    "**Memorable story**: In 2023, lawyers cited fake case law generated by ChatGPT in federal court (Mata v. Avianca), highlighting both the power and risks of generative AI in high-stakes applications.\n",
    "\n",
    "### Current Frontier (2024-2025)\n",
    "\n",
    "**Multimodal models** combine text, vision, and audio. **Retrieval-Augmented Generation** addresses hallucination problems. **Constitutional AI** and alignment research focus on safety and reliability.\n",
    "\n",
    "## References\n",
    "\n",
    "- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. *Journal of Machine Learning Research*, 3, 993-1022.\n",
    "- Brown, P. F., Cocke, J., Della Pietra, S. A., et al. (1990). A statistical approach to machine translation. *Computational Linguistics*, 16(2), 79-85.\n",
    "- Chomsky, N. (1957). *Syntactic Structures*. The Hague: Mouton.\n",
    "- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.\n",
    "- Hutchins, W. J. (1997). From first conception to first demonstration: the nascent years of machine translation, 1947-1954. *Machine Translation*, 12(3), 195-252.\n",
    "- Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. *Computational Linguistics*, 19(2), 313-330.\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. *arXiv preprint arXiv:1301.3781*.\n",
    "- O'Connor, B., Balasubramanyan, R., Routledge, B. R., & Smith, N. A. (2010). From tweets to polls: Linking text sentiment to public opinion time series. *Proceedings of ICWSM*, 11(122-129), 1-2.\n",
    "- Salton, G., & McGill, M. J. (1983). *Introduction to Modern Information Retrieval*. McGraw-Hill.\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.\n",
    "- Wu, Y., Schuster, M., Chen, Z., et al. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ts2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
