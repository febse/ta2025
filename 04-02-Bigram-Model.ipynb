{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9767324",
   "metadata": {},
   "source": [
    "# A Simple Bigram Language Model with PyTorch\n",
    "\n",
    "In this notebook, we'll build a simple bigram language model using Oscar Wilde quotes. A bigram model predicts the next word given the previous word.\n",
    "\n",
    "## What is a Bigram?\n",
    "\n",
    "A bigram is a sequence of two consecutive words. For example:\n",
    "- Sentence: \"The only way to deal with an unfree world is to become absolutely free\"\n",
    "- Bigrams: (\"The\", \"only\"), (\"only\", \"way\"), (\"way\", \"to\"), ...\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The bigram model consists of:\n",
    "1. **Embedding layer**: Maps word indices to dense vectors\n",
    "2. **Output layer**: Predicts logits for the next word\n",
    "\n",
    "Given word index $i$ (the current word), the model outputs logits for all possible next words.\n",
    "\n",
    "## Shape Tracking\n",
    "\n",
    "- Word index (input): scalar or $(batch\\_size,)$\n",
    "- Embedding: $(batch\\_size, embedding\\_dim)$\n",
    "- Output logits: $(batch\\_size, vocab\\_size)$\n",
    "- Target (next word): $(batch\\_size,)$ with integer indices\n",
    "- Loss: scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b33f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e7fbc7680b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d879c1c",
   "metadata": {},
   "source": [
    "## Create One-Hot Encoded Vectors\n",
    "\n",
    "Instead of using an embedding layer, we'll manually create one-hot encoded vectors for each word. A one-hot vector has a 1 in the position corresponding to the word and 0s elsewhere.\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "- For a vocabulary of size V, each word is represented as a vector of length V\n",
    "- Word at index $i$ is represented as a vector with 1 at position $i$ and 0 elsewhere\n",
    "- Shape: $(vocab\\_size,)$ for a single word, or $(batch\\_size, vocab\\_size)$ for a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1817ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAW TEXT\n",
      "======================================================================\n",
      "Quote 1: The only way to deal with an unfree world is to become absolutely free\n",
      "Quote 2: Be yourself; everyone else is already taken\n",
      "\n",
      "Combined text: the only way to deal with an unfree world is to become absolutely free be yourself; everyone else is already taken\n",
      "\n",
      "======================================================================\n",
      "TOKENIZATION\n",
      "======================================================================\n",
      "Tokens: ['the', 'only', 'way', 'to', 'deal', 'with', 'an', 'unfree', 'world', 'is', 'to', 'become', 'absolutely', 'free', 'be', 'yourself', 'everyone', 'else', 'is', 'already', 'taken']\n",
      "Number of tokens: 21\n",
      "\n",
      "Vocabulary size: 19\n",
      "Vocabulary: ['absolutely', 'already', 'an', 'be', 'become', 'deal', 'else', 'everyone', 'free', 'is', 'only', 'taken', 'the', 'to', 'unfree', 'way', 'with', 'world', 'yourself']\n",
      "\n",
      "======================================================================\n",
      "WORD-TO-INDEX MAPPING\n",
      "======================================================================\n",
      "  absolutely      ->  0\n",
      "  already         ->  1\n",
      "  an              ->  2\n",
      "  be              ->  3\n",
      "  become          ->  4\n",
      "  deal            ->  5\n",
      "  else            ->  6\n",
      "  everyone        ->  7\n",
      "  free            ->  8\n",
      "  is              ->  9\n",
      "  only            -> 10\n",
      "  taken           -> 11\n",
      "  the             -> 12\n",
      "  to              -> 13\n",
      "  unfree          -> 14\n",
      "  way             -> 15\n",
      "  with            -> 16\n",
      "  world           -> 17\n",
      "  yourself        -> 18\n"
     ]
    }
   ],
   "source": [
    "# Oscar Wilde quotes\n",
    "quote1 = \"The only way to deal with an unfree world is to become absolutely free\"\n",
    "quote2 = \"Be yourself; everyone else is already taken\"\n",
    "\n",
    "text = (quote1 + \" \" + quote2).lower()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAW TEXT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Quote 1: {quote1}\")\n",
    "print(f\"Quote 2: {quote2}\")\n",
    "print(f\"\\nCombined text: {text}\")\n",
    "\n",
    "# Tokenize: split by whitespace and remove punctuation from word boundaries\n",
    "# We'll treat punctuation as separate tokens\n",
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenization: split on whitespace\"\"\"\n",
    "    words = text.split()\n",
    "    # Remove punctuation from end of words but keep the word\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        # Remove trailing punctuation\n",
    "        word_clean = word.rstrip('.,;:!?')\n",
    "        if word_clean:\n",
    "            tokens.append(word_clean)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOKENIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "# Create word-to-index and index-to-word mappings\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WORD-TO-INDEX MAPPING\")\n",
    "print(\"=\" * 70)\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"  {word:15s} -> {idx:2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6964c87",
   "metadata": {},
   "source": [
    "## Create Bigrams and Build Training Data\n",
    "\n",
    "From the sequence of tokens, we create bigrams: each consecutive pair of words becomes a (input, target) example.\n",
    "\n",
    "**Shape Information:**\n",
    "- Input indices: $(n_{bigrams},)$ - each element is a word index\n",
    "- Target indices: $(n_{bigrams},)$ - each element is the next word index\n",
    "- Training pairs: Converted to PyTorch tensors for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a00fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN INDICES\n",
      "======================================================================\n",
      "Tokens:      ['the', 'only', 'way', 'to', 'deal', 'with', 'an', 'unfree', 'world', 'is', 'to', 'become', 'absolutely', 'free', 'be', 'yourself', 'everyone', 'else', 'is', 'already', 'taken']\n",
      "Indices:     [12, 10, 15, 13, 5, 16, 2, 14, 17, 9, 13, 4, 0, 8, 3, 18, 7, 6, 9, 1, 11]\n",
      "\n",
      "======================================================================\n",
      "BIGRAMS\n",
      "======================================================================\n",
      "Number of bigrams: 20\n",
      "\n",
      "Bigrams (as word pairs and indices):\n",
      "   0. (the          -> only        ) = (12 -> 10)\n",
      "   1. (only         -> way         ) = (10 -> 15)\n",
      "   2. (way          -> to          ) = (15 -> 13)\n",
      "   3. (to           -> deal        ) = (13 ->  5)\n",
      "   4. (deal         -> with        ) = ( 5 -> 16)\n",
      "   5. (with         -> an          ) = (16 ->  2)\n",
      "   6. (an           -> unfree      ) = ( 2 -> 14)\n",
      "   7. (unfree       -> world       ) = (14 -> 17)\n",
      "   8. (world        -> is          ) = (17 ->  9)\n",
      "   9. (is           -> to          ) = ( 9 -> 13)\n",
      "  10. (to           -> become      ) = (13 ->  4)\n",
      "  11. (become       -> absolutely  ) = ( 4 ->  0)\n",
      "  12. (absolutely   -> free        ) = ( 0 ->  8)\n",
      "  13. (free         -> be          ) = ( 8 ->  3)\n",
      "  14. (be           -> yourself    ) = ( 3 -> 18)\n",
      "  15. (yourself     -> everyone    ) = (18 ->  7)\n",
      "  16. (everyone     -> else        ) = ( 7 ->  6)\n",
      "  17. (else         -> is          ) = ( 6 ->  9)\n",
      "  18. (is           -> already     ) = ( 9 ->  1)\n",
      "  19. (already      -> taken       ) = ( 1 -> 11)\n",
      "\n",
      "======================================================================\n",
      "TRAINING DATA SHAPES (INDICES)\n",
      "======================================================================\n",
      "X_indices shape: (20,) - input word indices\n",
      "y_indices shape: (20,) - target word indices\n",
      "\n",
      "======================================================================\n",
      "ONE-HOT ENCODED TRAINING DATA\n",
      "======================================================================\n",
      "X_one_hot shape: (20, 19)\n",
      "y_indices shape: (20,)\n",
      "\n",
      "Example one-hot encoded vectors (first 3 bigrams):\n",
      "\n",
      "Bigram 1: 'the' -> 'only'\n",
      "  Input one-hot vector (indices with 1s): [12]\n",
      "  Input one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  Target index: 10\n",
      "\n",
      "Bigram 2: 'only' -> 'way'\n",
      "  Input one-hot vector (indices with 1s): [10]\n",
      "  Input one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  Target index: 15\n",
      "\n",
      "Bigram 3: 'way' -> 'to'\n",
      "  Input one-hot vector (indices with 1s): [15]\n",
      "  Input one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  Target index: 13\n",
      "\n",
      "======================================================================\n",
      "PYTORCH TENSOR SHAPES\n",
      "======================================================================\n",
      "X_one_hot_tensor shape: torch.Size([20, 19]), dtype: torch.float32\n",
      "y_one_hot_tensor shape: torch.Size([20]), dtype: torch.int64\n",
      "X_one_hot_tensor:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.]])\n",
      "y_one_hot_tensor: tensor([10, 15, 13,  5, 16,  2, 14, 17,  9, 13,  4,  0,  8,  3, 18,  7,  6,  9,\n",
      "         1, 11])\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to indices\n",
    "token_indices = [word2idx[word] for word in tokens]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN INDICES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Tokens:      {tokens}\")\n",
    "print(f\"Indices:     {token_indices}\")\n",
    "\n",
    "# Create bigrams\n",
    "bigrams = list(zip(token_indices[:-1], token_indices[1:]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BIGRAMS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Number of bigrams: {len(bigrams)}\")\n",
    "print(\"\\nBigrams (as word pairs and indices):\")\n",
    "for i, (input_idx, target_idx) in enumerate(bigrams):\n",
    "    input_word = idx2word[input_idx]\n",
    "    target_word = idx2word[target_idx]\n",
    "    print(f\"  {i:2d}. ({input_word:12s} -> {target_word:12s}) = ({input_idx:2d} -> {target_idx:2d})\")\n",
    "\n",
    "# Separate inputs and targets\n",
    "X_indices = np.array([bg[0] for bg in bigrams])\n",
    "y_indices = np.array([bg[1] for bg in bigrams])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING DATA SHAPES (INDICES)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"X_indices shape: {X_indices.shape} - input word indices\")\n",
    "print(f\"y_indices shape: {y_indices.shape} - target word indices\")\n",
    "\n",
    "# Create one-hot encoded vectors manually\n",
    "def create_one_hot(indices, vocab_size):\n",
    "    \"\"\"\n",
    "    Create one-hot encoded vectors from word indices.\n",
    "    \n",
    "    Args:\n",
    "        indices: Array of shape (n_samples,) with word indices\n",
    "        vocab_size: Size of vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        one_hot: Array of shape (n_samples, vocab_size) with one-hot vectors\n",
    "    \"\"\"\n",
    "    n_samples = len(indices)\n",
    "    one_hot = np.zeros((n_samples, vocab_size))\n",
    "    one_hot[np.arange(n_samples), indices] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "X_one_hot = create_one_hot(X_indices, vocab_size)\n",
    "y_indices_tensor = torch.from_numpy(y_indices).long()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ONE-HOT ENCODED TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"X_one_hot shape: {X_one_hot.shape}\")\n",
    "print(f\"y_indices shape: {y_indices.shape}\")\n",
    "print(f\"\\nExample one-hot encoded vectors (first 3 bigrams):\")\n",
    "\n",
    "for i in range(min(3, len(bigrams))):\n",
    "    input_word = idx2word[X_indices[i]]\n",
    "    target_word = idx2word[y_indices[i]]\n",
    "    \n",
    "    print(f\"\\nBigram {i+1}: '{input_word}' -> '{target_word}'\")\n",
    "    print(f\"  Input one-hot vector (indices with 1s): {np.where(X_one_hot[i] == 1)[0]}\")\n",
    "    print(f\"  Input one-hot: {X_one_hot[i]}\")\n",
    "    print(f\"  Target index: {y_indices[i]}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_one_hot_tensor = torch.from_numpy(X_one_hot).float()  # (n_bigrams, vocab_size)\n",
    "y_one_hot_tensor = y_indices_tensor.long()  # (n_bigrams,)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PYTORCH TENSOR SHAPES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"X_one_hot_tensor shape: {X_one_hot_tensor.shape}, dtype: {X_one_hot_tensor.dtype}\")\n",
    "print(f\"y_one_hot_tensor shape: {y_one_hot_tensor.shape}, dtype: {y_one_hot_tensor.dtype}\")\n",
    "print(f\"X_one_hot_tensor:\\n{X_one_hot_tensor}\")\n",
    "print(f\"y_one_hot_tensor: {y_one_hot_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b1653",
   "metadata": {},
   "source": [
    "## Define the Bigram Neural Network Model (One-Hot Version)\n",
    "\n",
    "Instead of using an embedding layer, the model takes one-hot encoded vectors as input and predicts logits for all vocabulary words.\n",
    "\n",
    "**Architecture:**\n",
    "- Input: One-hot encoded vector of shape $(vocab\\_size,)$\n",
    "- Linear layer: Maps from $(vocab\\_size,)$ to $(vocab\\_size,)$ (one-hot space to logit space)\n",
    "- Output: Logits of shape $(vocab\\_size,)$\n",
    "\n",
    "This is equivalent to learning a word transition matrix where each row represents the logits for the next word given the current word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e102ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BIGRAM MODEL ARCHITECTURE (ONE-HOT VERSION)\n",
      "======================================================================\n",
      "BigramModel(\n",
      "  (linear): Linear(in_features=19, out_features=19, bias=True)\n",
      ")\n",
      "\n",
      "======================================================================\n",
      "MODEL PARAMETERS AND SHAPES\n",
      "======================================================================\n",
      "linear.weight             shape: torch.Size([19, 19])\n",
      "linear.bias               shape: torch.Size([19])\n",
      "\n",
      "Total parameters: 380\n",
      "\n",
      "======================================================================\n",
      "PARAMETER DETAILS\n",
      "======================================================================\n",
      "Linear layer weights (transition matrix):\n",
      "  Shape: torch.Size([19, 19]) = (vocab_size, vocab_size) = (19, 19)\n",
      "  Total params: 361\n",
      "  Interpretation: Weight[i,j] = logit for word j given word i\n",
      "  Initial weights (first few):\n",
      "tensor([[ 0.1754,  0.1904, -0.0537],\n",
      "        [ 0.0585, -0.1057, -0.0269],\n",
      "        [ 0.0251, -0.0724,  0.0616]])\n",
      "\n",
      "Linear layer bias:\n",
      "  Shape: torch.Size([19]) = (19,)\n",
      "  Total params: 19\n",
      "  Interpretation: Bias term added to all logits\n",
      "  Initial bias:\n",
      "tensor([-0.1306, -0.2068,  0.0102,  0.1017,  0.0508,  0.0454, -0.1740, -0.2142,\n",
      "         0.0040,  0.2092,  0.1324, -0.1336, -0.0298, -0.1691, -0.1107,  0.0415,\n",
      "         0.1249,  0.1900, -0.2106])\n",
      "\n",
      "======================================================================\n",
      "SHAPE TRANSFORMATIONS\n",
      "======================================================================\n",
      "\n",
      "For a batch of size 20:\n",
      "  One-hot input shape: torch.Size([20, 19])\n",
      "  Weight matrix shape: torch.Size([19, 19])\n",
      "  Output logits shape: (20, 19)\n",
      "\n",
      "Computation: logits = one_hot @ weight^T + bias\n"
     ]
    }
   ],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram language model using one-hot encoded inputs.\n",
    "    \n",
    "    Given a one-hot encoded word vector, predicts logits for the next word.\n",
    "    \n",
    "    Architecture:\n",
    "    One-Hot Vector (vocab_size,) -> Linear (vocab_size, vocab_size) -> Logits (vocab_size,)\n",
    "    \n",
    "    This is equivalent to learning a transition matrix where element [i,j] \n",
    "    represents the logit for word j given word i.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Linear layer: takes one-hot vector of size vocab_size, outputs logits of size vocab_size\n",
    "        # Weight matrix: (vocab_size, vocab_size)\n",
    "        # This is the learned transition matrix\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, one_hot_input):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            one_hot_input: Tensor of shape (batch_size, vocab_size) with one-hot vectors\n",
    "                          or shape (vocab_size,) if single word\n",
    "            \n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, vocab_size) with output logits\n",
    "                   or shape (vocab_size,) if single word\n",
    "        \"\"\"\n",
    "        # Linear transformation\n",
    "        # Input: (batch_size, vocab_size) -> Output: (batch_size, vocab_size)\n",
    "        logits = self.linear(one_hot_input)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_epochs = 500\n",
    "\n",
    "# Initialize model\n",
    "model = BigramModel(vocab_size=vocab_size)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BIGRAM MODEL ARCHITECTURE (ONE-HOT VERSION)\")\n",
    "print(\"=\" * 70)\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL PARAMETERS AND SHAPES\")\n",
    "print(\"=\" * 70)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:25s} shape: {param.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "\n",
    "# Detailed parameter breakdown\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARAMETER DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "linear_weight = model.linear.weight\n",
    "linear_bias = model.linear.bias\n",
    "\n",
    "print(f\"Linear layer weights (transition matrix):\")\n",
    "print(f\"  Shape: {linear_weight.shape} = (vocab_size, vocab_size) = ({vocab_size}, {vocab_size})\")\n",
    "print(f\"  Total params: {linear_weight.numel()}\")\n",
    "print(f\"  Interpretation: Weight[i,j] = logit for word j given word i\")\n",
    "print(f\"  Initial weights (first few):\\n{linear_weight.data[:3, :3]}\")\n",
    "\n",
    "print(f\"\\nLinear layer bias:\")\n",
    "print(f\"  Shape: {linear_bias.shape} = ({vocab_size},)\")\n",
    "print(f\"  Total params: {linear_bias.numel()}\")\n",
    "print(f\"  Interpretation: Bias term added to all logits\")\n",
    "print(f\"  Initial bias:\\n{linear_bias.data}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAPE TRANSFORMATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFor a batch of size {len(X_one_hot_tensor)}:\")\n",
    "print(f\"  One-hot input shape: {X_one_hot_tensor.shape}\")\n",
    "print(f\"  Weight matrix shape: {linear_weight.shape}\")\n",
    "print(f\"  Output logits shape: {X_one_hot_tensor.shape[0], vocab_size}\")\n",
    "print(f\"\\nComputation: logits = one_hot @ weight^T + bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036d0ca",
   "metadata": {},
   "source": [
    "## Test Forward Pass Before Training\n",
    "\n",
    "Let's verify the shapes before training to ensure everything works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce32e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST FORWARD PASS\n",
      "======================================================================\n",
      "\n",
      "Test input: word 'absolutely' with index 0\n",
      "One-hot vector: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Input tensor shape: (19,)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST FORWARD PASS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with single word\n",
    "test_word_index = 0  # \"the\" (word at index 0)\n",
    "test_word = idx2word[test_word_index]\n",
    "\n",
    "# Create one-hot vector for this word\n",
    "test_one_hot = np.zeros(vocab_size)\n",
    "test_one_hot[test_word_index] = 1.0\n",
    "\n",
    "print(f\"\\nTest input: word '{test_word}' with index {test_word_index}\")\n",
    "print(f\"One-hot vector: {test_one_hot}\")\n",
    "print(f\"Input tensor shape: ({vocab_size},)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Single word one-hot\n",
    "    input_single = torch.from_numpy(test_one_hot).float().unsqueeze(0)  # Shape: (1, vocab_size)\n",
    "    logits = model(input_single)  # Shape: (1, vocab_size)\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Logits: {logits}\")\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    print(f\"\\nProbabilities shape: {probs.shape}\")\n",
    "    print(f\"Sum of probabilities: {probs.sum().item():.6f}\")\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_probs, top_indices = torch.topk(probs, 3, dim=-1)\n",
    "    print(f\"\\nTop 3 predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
    "        next_word = idx2word[idx.item()]\n",
    "        print(f\"  {i+1}. '{next_word}' (index {idx.item()}) with probability {prob.item():.4f}\")\n",
    "\n",
    "# Test with batch\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST WITH BATCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "batch_one_hot = X_one_hot_tensor[:3]  # First 3 training examples\n",
    "print(f\"\\nBatch input shape: {batch_one_hot.shape}\")\n",
    "print(f\"Input words: {[idx2word[X_indices[i]] for i in range(3)]}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_logits = model(batch_one_hot)\n",
    "    print(f\"Output logits shape: {batch_logits.shape}\")\n",
    "    print(f\"Output logits:\\n{batch_logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d9f9b",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We'll train the bigram model using CrossEntropyLoss, which expects:\n",
    "- Model output: logits of shape $(batch\\_size, vocab\\_size)$\n",
    "- Target: integer word indices of shape $(batch\\_size,)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e515b902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Loss function: CrossEntropyLoss\n",
      "Optimizer: Adam with learning rate 0.1\n",
      "Number of epochs: 500\n",
      "Number of training examples (bigrams): 20\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "Epoch  50/500 | Loss: 0.171885\n",
      "Epoch 100/500 | Loss: 0.153075\n",
      "Epoch 150/500 | Loss: 0.147764\n",
      "Epoch 200/500 | Loss: 0.145013\n",
      "Epoch 250/500 | Loss: 0.143387\n",
      "Epoch 300/500 | Loss: 0.142337\n",
      "Epoch 350/500 | Loss: 0.141616\n",
      "Epoch 400/500 | Loss: 0.141096\n",
      "Epoch 450/500 | Loss: 0.140707\n",
      "Epoch 500/500 | Loss: 0.140407\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n",
      "Final loss: 0.140407\n",
      "Initial loss: 2.941630\n",
      "Loss reduction: 2.801223\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam with learning rate {learning_rate}\")\n",
    "print(f\"Number of epochs: {n_epochs}\")\n",
    "print(f\"Number of training examples (bigrams): {len(X_one_hot_tensor)}\")\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(X_one_hot_tensor)  # Shape: (n_bigrams, vocab_size)\n",
    "    loss = criterion(logits, y_one_hot_tensor)  # Shape: scalar\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"Initial loss: {losses[0]:.6f}\")\n",
    "print(f\"Loss reduction: {losses[0] - losses[-1]:.6f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Cross Entropy)')\n",
    "plt.title('Bigram Model Training Loss (One-Hot Input)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303af430",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model\n",
    "\n",
    "Now let's use the trained model to predict the next word given a current word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b03ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL EVALUATION ON TRAINING DATA\n",
      "======================================================================\n",
      "Accuracy: 0.9000 (90.00%)\n",
      "\n",
      "======================================================================\n",
      "DETAILED PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "1. Input: 'the' -> True next: 'only'\n",
      "   Predicted: 'only' (probability: 0.9982) \n",
      "   True word probability: 0.9982\n",
      "   Top 3 predictions:\n",
      "      1. 'only' (0.9982)\n",
      "      2. 'to' (0.0002)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "2. Input: 'only' -> True next: 'way'\n",
      "   Predicted: 'way' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'way' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0001)\n",
      "\n",
      "3. Input: 'way' -> True next: 'to'\n",
      "   Predicted: 'to' (probability: 0.9984) \n",
      "   True word probability: 0.9984\n",
      "   Top 3 predictions:\n",
      "      1. 'to' (0.9984)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'deal' (0.0001)\n",
      "\n",
      "4. Input: 'to' -> True next: 'deal'\n",
      "   Predicted: 'become' (probability: 0.4992) \n",
      "   True word probability: 0.4992\n",
      "   Top 3 predictions:\n",
      "      1. 'become' (0.4992)\n",
      "      2. 'deal' (0.4992)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "5. Input: 'deal' -> True next: 'with'\n",
      "   Predicted: 'with' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'with' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0002)\n",
      "\n",
      "6. Input: 'with' -> True next: 'an'\n",
      "   Predicted: 'an' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'an' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0001)\n",
      "\n",
      "7. Input: 'an' -> True next: 'unfree'\n",
      "   Predicted: 'unfree' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'unfree' (0.9981)\n",
      "      2. 'to' (0.0002)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "8. Input: 'unfree' -> True next: 'world'\n",
      "   Predicted: 'world' (probability: 0.9982) \n",
      "   True word probability: 0.9982\n",
      "   Top 3 predictions:\n",
      "      1. 'world' (0.9982)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'else' (0.0001)\n",
      "\n",
      "9. Input: 'world' -> True next: 'is'\n",
      "   Predicted: 'is' (probability: 0.9983) \n",
      "   True word probability: 0.9983\n",
      "   Top 3 predictions:\n",
      "      1. 'is' (0.9983)\n",
      "      2. 'to' (0.0001)\n",
      "      3. 'yourself' (0.0001)\n",
      "\n",
      "10. Input: 'is' -> True next: 'to'\n",
      "   Predicted: 'to' (probability: 0.4994) \n",
      "   True word probability: 0.4994\n",
      "   Top 3 predictions:\n",
      "      1. 'to' (0.4994)\n",
      "      2. 'already' (0.4991)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "11. Input: 'to' -> True next: 'become'\n",
      "   Predicted: 'become' (probability: 0.4992) \n",
      "   True word probability: 0.4992\n",
      "   Top 3 predictions:\n",
      "      1. 'become' (0.4992)\n",
      "      2. 'deal' (0.4992)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "12. Input: 'become' -> True next: 'absolutely'\n",
      "   Predicted: 'absolutely' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'absolutely' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0001)\n",
      "\n",
      "13. Input: 'absolutely' -> True next: 'free'\n",
      "   Predicted: 'free' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'free' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0002)\n",
      "\n",
      "14. Input: 'free' -> True next: 'be'\n",
      "   Predicted: 'be' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'be' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'unfree' (0.0001)\n",
      "\n",
      "15. Input: 'be' -> True next: 'yourself'\n",
      "   Predicted: 'yourself' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'yourself' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0002)\n",
      "\n",
      "16. Input: 'yourself' -> True next: 'everyone'\n",
      "   Predicted: 'everyone' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'everyone' (0.9981)\n",
      "      2. 'to' (0.0002)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "17. Input: 'everyone' -> True next: 'else'\n",
      "   Predicted: 'else' (probability: 0.9981) \n",
      "   True word probability: 0.9981\n",
      "   Top 3 predictions:\n",
      "      1. 'else' (0.9981)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0001)\n",
      "\n",
      "18. Input: 'else' -> True next: 'is'\n",
      "   Predicted: 'is' (probability: 0.9984) \n",
      "   True word probability: 0.9984\n",
      "   Top 3 predictions:\n",
      "      1. 'is' (0.9984)\n",
      "      2. 'to' (0.0001)\n",
      "      3. 'taken' (0.0001)\n",
      "\n",
      "19. Input: 'is' -> True next: 'already'\n",
      "   Predicted: 'to' (probability: 0.4994) \n",
      "   True word probability: 0.4991\n",
      "   Top 3 predictions:\n",
      "      1. 'to' (0.4994)\n",
      "      2. 'already' (0.4991)\n",
      "      3. 'is' (0.0002)\n",
      "\n",
      "20. Input: 'already' -> True next: 'taken'\n",
      "   Predicted: 'taken' (probability: 0.9982) \n",
      "   True word probability: 0.9982\n",
      "   Top 3 predictions:\n",
      "      1. 'taken' (0.9982)\n",
      "      2. 'is' (0.0002)\n",
      "      3. 'to' (0.0001)\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for all training bigrams\n",
    "with torch.no_grad():\n",
    "    logits_all = model(X_one_hot_tensor)  # Shape: (n_bigrams, vocab_size)\n",
    "    predictions = torch.argmax(logits_all, dim=-1)  # Shape: (n_bigrams,)\n",
    "    probs_all = torch.softmax(logits_all, dim=-1)  # Shape: (n_bigrams, vocab_size)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == y_one_hot_tensor).float().mean().item()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION ON TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Show detailed predictions\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(bigrams):\n",
    "    input_word = idx2word[input_idx]\n",
    "    target_word = idx2word[target_idx]\n",
    "    pred_idx = predictions[i].item()\n",
    "    pred_word = idx2word[pred_idx]\n",
    "    pred_prob = probs_all[i, pred_idx].item()\n",
    "    target_prob = probs_all[i, target_idx].item()\n",
    "    \n",
    "    correct = \"\" if pred_idx == target_idx else \"\"\n",
    "    \n",
    "    print(f\"\\n{i+1}. Input: '{input_word}' -> True next: '{target_word}'\")\n",
    "    print(f\"   Predicted: '{pred_word}' (probability: {pred_prob:.4f}) {correct}\")\n",
    "    print(f\"   True word probability: {target_prob:.4f}\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    top_probs, top_indices = torch.topk(probs_all[i], 3)\n",
    "    print(f\"   Top 3 predictions:\")\n",
    "    for rank, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        word = idx2word[idx.item()]\n",
    "        print(f\"      {rank+1}. '{word}' ({prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161eac5b",
   "metadata": {},
   "source": [
    "## Generate Text Using the Model\n",
    "\n",
    "Let's generate new text by starting with a word and repeatedly predicting the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, idx2word, word2idx, start_word, max_length=10):\n",
    "    \"\"\"\n",
    "    Generate text using the bigram model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained bigram model\n",
    "        idx2word: Index to word mapping\n",
    "        word2idx: Word to index mapping\n",
    "        start_word: Starting word for generation\n",
    "        max_length: Maximum number of words to generate\n",
    "        \n",
    "    Returns:\n",
    "        Generated text as a string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    generated_words = [start_word]\n",
    "    current_word = start_word\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - 1):\n",
    "            # Get current word index\n",
    "            current_idx = word2idx[current_word]\n",
    "            \n",
    "            # Create one-hot vector\n",
    "            one_hot = np.zeros(len(word2idx))\n",
    "            one_hot[current_idx] = 1.0\n",
    "            one_hot_tensor = torch.from_numpy(one_hot).float().unsqueeze(0)\n",
    "            \n",
    "            # Get logits for next word\n",
    "            logits = model(one_hot_tensor)  # Shape: (1, vocab_size)\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (1, vocab_size)\n",
    "            \n",
    "            # Sample or take argmax\n",
    "            next_idx = torch.argmax(probs, dim=-1).item()\n",
    "            next_word = idx2word[next_idx]\n",
    "            \n",
    "            generated_words.append(next_word)\n",
    "            current_word = next_word\n",
    "    \n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "# Generate text starting from different words\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_words = [\"the\", \"be\", \"yourself\", \"world\"]\n",
    "\n",
    "for start_word in start_words:\n",
    "    if start_word in word2idx:\n",
    "        generated = generate_text(model, idx2word, word2idx, start_word, max_length=8)\n",
    "        print(f\"\\nStarting with '{start_word}':\")\n",
    "        print(f\"  Generated: {generated}\")\n",
    "    else:\n",
    "        print(f\"\\nWord '{start_word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fde1f",
   "metadata": {},
   "source": [
    "## Inspect Learned Embeddings\n",
    "\n",
    "Let's examine the embedding vectors learned for each word to see if similar words have similar embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b262c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LEARNED TRANSITION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the learned weight matrix (this is the transition matrix)\n",
    "transition_matrix = model.linear.weight.detach().numpy()  # Shape: (vocab_size, vocab_size)\n",
    "bias = model.linear.bias.detach().numpy()  # Shape: (vocab_size,)\n",
    "\n",
    "print(f\"\\nTransition matrix shape: {transition_matrix.shape}\")\n",
    "print(f\"Interpretation:\")\n",
    "print(f\"  Rows represent input words (current word)\")\n",
    "print(f\"  Columns represent output words (next word)\")\n",
    "print(f\"  Element [i,j] = logit for word j given word i\")\n",
    "\n",
    "print(f\"\\nTransition matrix:\")\n",
    "print(f\"{'Word':15s}\", end=\"\")\n",
    "for col_word in vocab:\n",
    "    print(f\"{col_word:8s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (15 + 8 * len(vocab)))\n",
    "\n",
    "for i, row_word in enumerate(vocab):\n",
    "    print(f\"{row_word:15s}\", end=\"\")\n",
    "    for j in range(len(vocab)):\n",
    "        logit = transition_matrix[i, j]\n",
    "        # Convert logit to probability for readability\n",
    "        all_logits = transition_matrix[i, :]\n",
    "        prob = np.exp(logit) / np.sum(np.exp(all_logits))\n",
    "        print(f\"{prob:8.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nBias term:\")\n",
    "for i, word in enumerate(vocab):\n",
    "    print(f\"  {word:15s}: {bias[i]:.6f}\")\n",
    "\n",
    "# Analyze what the model learned\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMost likely next word for each word:\")\n",
    "for i, word in enumerate(vocab):\n",
    "    # Get the row for this word\n",
    "    logits = transition_matrix[i, :] + bias\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    \n",
    "    # Get top 3\n",
    "    top_indices = np.argsort(probs)[-3:][::-1]\n",
    "    \n",
    "    print(f\"\\n  Given '{word}':\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        next_word = vocab[idx]\n",
    "        prob = probs[idx]\n",
    "        print(f\"    {rank}. '{next_word}' ({prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887590c9",
   "metadata": {},
   "source": [
    "## Summary: One-Hot Bigram Model Architecture\n",
    "\n",
    "### Model Flow\n",
    "\n",
    "$$\n",
    "\\text{Word Index} \\xrightarrow{\\text{One-Hot}}  \\text{One-Hot Vector} \\xrightarrow{\\text{Linear}(V, V)} \\text{Logits}\n",
    "$$\n",
    "\n",
    "where $V$ is the vocabulary size.\n",
    "\n",
    "### Shape Transformations\n",
    "\n",
    "For a batch of $B$ words:\n",
    "\n",
    "| Component | Shape | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Input (word indices) | $(B,)$ | B word indices |\n",
    "| One-hot encoded | $(B, V)$ | B one-hot vectors of size V |\n",
    "| Output logits | $(B, V)$ | Logits for next word prediction |\n",
    "| Softmax (probabilities) | $(B, V)$ | Probability distribution over vocabulary |\n",
    "| Predictions (argmax) | $(B,)$ | Predicted next word indices |\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "For a bigram model with vocabulary size $V$:\n",
    "\n",
    "$$\n",
    "\\text{Total parameters} = \\underbrace{V \\times V}_{\\text{Linear weight}} + \\underbrace{V}_{\\text{Linear bias}} = V(V + 1)\n",
    "$$\n",
    "\n",
    "### Key Difference from Embedding Version\n",
    "\n",
    "**Embedding Layer Approach:**\n",
    "- Word index  Dense embedding vector $(embedding\\_dim,)$  Logits\n",
    "- More compact representation, embeddings can capture semantic similarity\n",
    "- Total parameters: $V \\times embedding\\_dim + embedding\\_dim \\times V + V$\n",
    "\n",
    "**One-Hot Approach:**\n",
    "- Word index  One-hot vector $(V,)$  Logits\n",
    "- Full-rank representation, direct access to all information\n",
    "- Total parameters: $V \\times V + V$\n",
    "- Weight matrix is a learned transition matrix\n",
    "\n",
    "### Interpretation: Transition Matrix\n",
    "\n",
    "The learned weight matrix is a **transition matrix** where:\n",
    "- Row $i$ represents logits for all possible next words given word $i$\n",
    "- Element $[i,j]$ represents the learned likelihood of word $j$ following word $i$\n",
    "- After softmax, this gives probability distribution over next words\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**One-Hot Approach:**\n",
    "-  Simple and interpretable - direct word-to-word relationships\n",
    "-  Full-rank representation preserves all information\n",
    "-  More parameters for large vocabularies ($V^2$ vs $V \\times embedding\\_dim$)\n",
    "-  No learned semantic similarity - each word is orthogonal\n",
    "\n",
    "**Embedding Approach:**\n",
    "-  Fewer parameters for large vocabularies\n",
    "-  Can capture semantic relationships between words\n",
    "-  Less interpretable - embeddings are abstract\n",
    "-  May lose information when embedding dimension is too small\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For input one-hot vector $\\mathbf{e}_i$ (1 at position $i$, 0 elsewhere):\n",
    "\n",
    "$$\n",
    "\\text{logits} = \\mathbf{e}_i^T W + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{V \\times V}$ is the transition matrix and $\\mathbf{b} \\in \\mathbb{R}^V$ is the bias term.\n",
    "\n",
    "This is equivalent to:\n",
    "$$\n",
    "\\text{logits} = W[i, :] + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "i.e., selecting row $i$ from the weight matrix.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "This simple one-hot bigram model demonstrates:\n",
    "1. **Language modeling fundamentals**: Predicting next words from context\n",
    "2. **Matrix interpretation**: Weight matrix as learned relationships\n",
    "3. **One-hot encoding**: Basic representation for categorical variables\n",
    "4. **Extension to n-grams**: Can be extended to longer contexts by concatenating one-hot vectors\n",
    "5. **Building blocks**: Foundation for more complex sequence models (RNNs, Transformers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
