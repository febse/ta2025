{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "\n",
    "Open in [Google Colab](https://colab.research.google.com/github/febse/ta2025/blob/main/03-01-Word-Embeddings-Overview.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gensim if needed (Colab)\n",
    "try:\n",
    "    import gensim\n",
    "except ImportError:\n",
    "    %pip install gensim\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the Google News Word2Vec model (300d, English)\n",
    "model = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mking\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now our approach to representing words has been treating them as discrete symbols. Each word was represented as a one-hot vector, i.e., a vector of length V (the size of the vocabulary) with a 1 in the position corresponding to the word and 0s elsewhere. This representation has several limitations:\n",
    "\n",
    "- The length of the vector is equal to the size of the vocabulary, which can be very large (tens or hundreds of thousands of words).\n",
    "- This representation cannot capture any relationships between the words (all vectors are orthogonal to each other) and hence cannot capture semantic or syntactic similarities between words.\n",
    "\n",
    "As a solution to the first problem we can use dense vector representations of words, i.e., vectors of much smaller size (e.g., 100 or 300 dimensions) where each dimension can take any real value. Such dense vector representations are called **word embeddings**. We actually already saw one method that in effect compressed the sparse one-hot vectors into dense vectors when we applied SVD to the term-document matrix in Latent Semantic Analysis (LSA).\n",
    "\n",
    "However, these word embeddings are unlikely to capture semantic relationships between words if they are learned in isolation.\n",
    "\n",
    "The word2vec model [@MIKOLOV2013EfficientEstimationWord] turned out to deliver impressive results by learning word embeddings based on the context in which words appear. There are two variants of the word2vec model: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "The skip-gram model tries to predict the context words given a target word. The architecture of the skip-gram model is shown in the figure below.\n",
    "\n",
    "For example in a sentence \"The cat sits on the mat\", if the target word is \"sits\", the context words could be \"The\", \"cat\", \"on\", \"the\", \"mat\". The model takes the one-hot vector of the target word as input, passes it through a hidden layer (which is essentially a weight matrix that transforms the one-hot vector into a dense vector), and then tries to predict the one-hot vectors of the context words.\n",
    "\n",
    "![Skip-gram Model](./figures/skip-gram-architecture.webp)\n",
    "\n",
    "To make sense of this architecture, let's consider some simpler models that will help us understand how the skip-gram model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
