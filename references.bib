@article{BLEI2003LatentDirichletAllocation,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  date = {2003},
  journaltitle = {Journal of machine Learning research},
  volume = {3},
  pages = {993--1022},
  url = {https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
  issue = {Jan}
}

@report{BOEING2024AnatomyChineseInnovation,
  type = {Working Papers},
  title = {The Anatomy of Chinese Innovation: {{Insights}} on Patent Quality and Ownership},
  author = {Boeing, Philipp and Brandt, Loren and Dai, Ruochen and Lim, Kevin and Peters, Bettina},
  date = {2024-03},
  number = {tecipa-770},
  institution = {University of Toronto, Department of Economics},
  url = {https://ideas.repec.org/p/tor/tecipa/tecipa-770.html},
  abstract = {We study the evolution of patenting in China from 1985-2019. We use a Large Language Model to measure patent importance based on patent abstracts and classify patent ownership using a comprehensive business registry. We highlight four insights. First, average patent importance declined from 2000-2010 but has increased more recently. Second, private Chinese firms account for most of patenting growth whereas overseas patentees have played a diminishing role. Third, patentees have greatly reduced their dependence on foreign knowledge. Finally, Chinese and foreign patenting have become more similar in technological composition, but differences persist within technology classes as revealed by abstract similarities.},
  keywords = {Innovation Patents Technology China}
}

@inproceedings{devlin-etal-2019-bert,
  title = {{{BERT}}: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{marcus1993building,
  title = {Building a Large Annotated Corpus of English: {{The}} Penn Treebank},
  author = {Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  date = {1993},
  journaltitle = {Computational linguistics},
  volume = {19},
  number = {2},
  pages = {313--330},
  url = {https://aclanthology.org/J93-2004.pdf}
}

@article{MIKOLOV2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  urldate = {2023-11-29},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  version = {3},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@inproceedings{PENNINGTON2014GloveGlobalVectors,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  urldate = {2023-12-04},
  eventtitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english}
}

@book{SALTON1986IntroductionModernInformation,
  title = {Introduction to Modern Information Retrieval},
  author = {Salton, Gerard and McGill, Michael J.},
  date = {1986},
  publisher = {McGraw-Hill, Inc.},
  location = {USA},
  url = {https://archive.org/details/introductiontomo00salt/page/n9/mode/2up},
  isbn = {0-07-054484-0},
  file = {/home/amarov/Downloads/e-books/text/Salton and McGill - 1986 - Introduction to modern information retrieval.djvu}
}
