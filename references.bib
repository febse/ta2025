@article{BLEI2003LatentDirichletAllocation,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  date = {2003},
  journaltitle = {Journal of machine Learning research},
  volume = {3},
  pages = {993--1022},
  url = {https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
  issue = {Jan}
}

@report{BOEING2024AnatomyChineseInnovation,
  type = {Working Papers},
  title = {The Anatomy of Chinese Innovation: {{Insights}} on Patent Quality and Ownership},
  author = {Boeing, Philipp and Brandt, Loren and Dai, Ruochen and Lim, Kevin and Peters, Bettina},
  date = {2024-03},
  number = {tecipa-770},
  institution = {University of Toronto, Department of Economics},
  url = {https://ideas.repec.org/p/tor/tecipa/tecipa-770.html},
  abstract = {We study the evolution of patenting in China from 1985-2019. We use a Large Language Model to measure patent importance based on patent abstracts and classify patent ownership using a comprehensive business registry. We highlight four insights. First, average patent importance declined from 2000-2010 but has increased more recently. Second, private Chinese firms account for most of patenting growth whereas overseas patentees have played a diminishing role. Third, patentees have greatly reduced their dependence on foreign knowledge. Finally, Chinese and foreign patenting have become more similar in technological composition, but differences persist within technology classes as revealed by abstract similarities.},
  keywords = {Innovation Patents Technology China}
}

@inproceedings{devlin-etal-2019-bert,
  title = {{{BERT}}: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{LWIN2020GlobalSentimentsSurrounding,
  title = {Global {{Sentiments Surrounding}} the {{COVID-19 Pandemic}} on {{Twitter}}: {{Analysis}} of {{Twitter Trends}}},
  shorttitle = {Global {{Sentiments Surrounding}} the {{COVID-19 Pandemic}} on {{Twitter}}},
  author = {Lwin, May Oo and Lu, Jiahui and Sheldenkar, Anita and Schulz, Peter Johannes and Shin, Wonsun and Gupta, Raj and Yang, Yinping},
  date = {2020-05-22},
  journaltitle = {JMIR Public Health and Surveillance},
  shortjournal = {JMIR Public Health Surveill},
  volume = {6},
  number = {2},
  pages = {e19447},
  issn = {2369-2960},
  doi = {10.2196/19447},
  url = {http://publichealth.jmir.org/2020/2/e19447/},
  urldate = {2025-11-23},
  abstract = {Background               With the World Health Organization’s pandemic declaration and government-initiated actions against coronavirus disease (COVID-19), sentiments surrounding COVID-19 have evolved rapidly.                                         Objective               This study aimed to examine worldwide trends of four emotions—fear, anger, sadness, and joy—and the narratives underlying those emotions during the COVID-19 pandemic.                                         Methods               Over 20 million social media twitter posts made during the early phases of the COVID-19 outbreak from January 28 to April 9, 2020, were collected using “wuhan,” “corona,” “nCov,” and “covid” as search keywords.                                         Results               Public emotions shifted strongly from fear to anger over the course of the pandemic, while sadness and joy also surfaced. Findings from word clouds suggest that fears around shortages of COVID-19 tests and medical supplies became increasingly widespread discussion points. Anger shifted from xenophobia at the beginning of the pandemic to discourse around the stay-at-home notices. Sadness was highlighted by the topics of losing friends and family members, while topics related to joy included words of gratitude and good health.                                         Conclusions               Overall, global COVID-19 sentiments have shown rapid evolutions within just the span of a few weeks. Findings suggest that emotion-driven collective issues around shared public distress experiences of the COVID-19 pandemic are developing and include large-scale social isolation and the loss of human lives. The steady rise of societal concerns indicated by negative emotions needs to be monitored and controlled by complementing regular crisis communication with strategic public health communication that aims to balance public psychological wellbeing.},
  langid = {english},
  file = {/home/amarov/Zotero/storage/VZBPAMT3/Lwin et al. - 2020 - Global Sentiments Surrounding the COVID-19 Pandemic on Twitter Analysis of Twitter Trends.pdf}
}

@article{marcus1993building,
  title = {Building a Large Annotated Corpus of English: {{The}} Penn Treebank},
  author = {Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  date = {1993},
  journaltitle = {Computational linguistics},
  volume = {19},
  number = {2},
  pages = {313--330},
  url = {https://aclanthology.org/J93-2004.pdf}
}

@inproceedings{mihalcea2004textrank,
  title = {Textrank: {{Bringing}} Order into Text},
  booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
  author = {Mihalcea, Rada and Tarau, Paul},
  date = {2004},
  pages = {404--411},
  url = {https://aclanthology.org/W04-3252.pdf}
}

@article{MIHALIC2015TourismCitizenshipRights,
  title = {Tourism and {{Citizenship}}. {{Rights}}, {{Freedoms}} and {{Responsibilities}} in the {{Global Order}} ({{Contemporary Geographies}} of {{Leisure}}, {{Tourism}} and {{Mobility}})},
  author = {Mihalič, Tanja},
  date = {2015-06},
  journaltitle = {Tourism Management},
  shortjournal = {Tourism Management},
  volume = {48},
  pages = {283--284},
  issn = {02615177},
  doi = {10.1016/j.tourman.2014.12.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0261517714002428},
  urldate = {2025-11-23},
  langid = {english}
}

@article{MIKOLOV2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  urldate = {2023-11-29},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  version = {3},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@inproceedings{PENNINGTON2014GloveGlobalVectors,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  urldate = {2023-12-04},
  eventtitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english}
}

@book{SALTON1986IntroductionModernInformation,
  title = {Introduction to Modern Information Retrieval},
  author = {Salton, Gerard and McGill, Michael J.},
  date = {1986},
  publisher = {McGraw-Hill, Inc.},
  location = {USA},
  url = {https://archive.org/details/introductiontomo00salt/page/n9/mode/2up},
  isbn = {0-07-054484-0},
  file = {/home/amarov/Downloads/e-books/text/Salton and McGill - 1986 - Introduction to modern information retrieval.djvu}
}
