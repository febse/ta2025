{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f540b577",
   "metadata": {},
   "source": [
    "## Using Word Embeddings\n",
    "\n",
    "In the previous sections, we have built a Word2Vec model from scratch using NumPy. Now, let's see how we can use the learned word embeddings for various NLP tasks with the focus on sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012e25fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./.venv/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.venv/lib/python3.13/site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./.venv/lib/python3.13/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in ./.venv/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the pretrained Word2Vec vectors (GoogleNews-vectors-negative300)\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f157c5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.46972656e-02, -2.41088867e-02, -8.78906250e-03,  1.91406250e-01,\n",
       "       -6.05468750e-02,  6.73828125e-02, -1.09863281e-02, -9.81445312e-02,\n",
       "        3.22570801e-02,  2.62451172e-02, -5.65185547e-02, -2.45117188e-01,\n",
       "       -1.46362305e-01, -2.02148438e-01, -1.06689453e-01,  6.86035156e-02,\n",
       "        6.16455078e-02,  1.58935547e-01,  7.89794922e-02, -5.77392578e-02,\n",
       "       -1.19506836e-01,  1.08154297e-01,  2.25585938e-01,  6.68334961e-02,\n",
       "        3.12500000e-02,  7.65380859e-02, -2.17895508e-01, -1.10351562e-01,\n",
       "       -9.94873047e-03, -1.67236328e-02, -1.26953125e-02,  1.56250000e-01,\n",
       "        6.70166016e-02,  3.39355469e-02,  5.46875000e-02,  6.81762695e-02,\n",
       "       -1.03027344e-01,  9.21020508e-02, -2.90527344e-02,  1.15600586e-01,\n",
       "        9.19494629e-02, -7.56835938e-02,  2.07031250e-01,  5.43823242e-02,\n",
       "        6.46362305e-02,  7.95898438e-02,  1.25732422e-01, -1.62109375e-01,\n",
       "        4.88281250e-03, -4.35791016e-02,  6.48803711e-02,  1.57470703e-01,\n",
       "        4.19921875e-02,  1.41113281e-01,  1.12304688e-02,  2.00195312e-01,\n",
       "       -7.94677734e-02,  7.14111328e-02,  8.30078125e-02, -1.29882812e-01,\n",
       "       -1.66625977e-02,  5.04455566e-02, -2.11914062e-01, -5.18798828e-02,\n",
       "       -9.35058594e-02, -1.41357422e-01, -7.70263672e-02,  7.34863281e-02,\n",
       "       -1.20117188e-01,  3.29589844e-02,  3.94287109e-02,  5.48095703e-02,\n",
       "        2.83813477e-02, -3.32031250e-02, -2.64160156e-01, -9.47265625e-02,\n",
       "        9.44824219e-02,  6.98852539e-02,  4.50248718e-02,  1.36108398e-01,\n",
       "       -8.11767578e-02, -2.41699219e-02,  4.87060547e-02, -8.27026367e-02,\n",
       "       -1.29486084e-01,  3.63769531e-02, -4.82177734e-02,  2.58789062e-01,\n",
       "       -5.05371094e-02, -9.38720703e-02,  6.21337891e-02,  1.70410156e-01,\n",
       "       -1.02203369e-01,  1.05895996e-02, -3.03955078e-02,  2.39257812e-02,\n",
       "        3.41796875e-03,  2.01416016e-02,  1.32812500e-01, -5.29785156e-02,\n",
       "       -1.41113281e-01,  3.54919434e-02,  7.56835938e-02,  1.49902344e-01,\n",
       "       -9.48486328e-02,  1.97265625e-01, -1.06689453e-01,  5.97534180e-02,\n",
       "        7.44628906e-03, -1.13159180e-01, -1.76513672e-01, -1.30554199e-01,\n",
       "        6.94274902e-03,  2.24914551e-02,  2.43652344e-01, -4.93774414e-02,\n",
       "       -6.02416992e-02, -1.69921875e-01,  5.34057617e-02, -1.84570312e-01,\n",
       "       -7.32421875e-03,  3.00903320e-02, -9.72824097e-02,  1.25976562e-01,\n",
       "        3.22875977e-02, -5.37109375e-02, -2.37792969e-01,  4.39453125e-02,\n",
       "        8.59375000e-02,  7.32421875e-03, -1.48681641e-01, -1.73828125e-01,\n",
       "       -1.44287109e-01, -1.05895996e-02, -1.13281250e-01, -7.32421875e-02,\n",
       "       -4.88281250e-04,  9.09423828e-03,  1.02722168e-01,  3.34167480e-02,\n",
       "        4.67529297e-02, -1.20971680e-01,  1.54052734e-01, -1.68457031e-02,\n",
       "       -3.71704102e-02,  7.07550049e-02, -6.62231445e-02, -7.78808594e-02,\n",
       "       -4.71801758e-02, -2.91748047e-02,  1.71630859e-01,  3.32031250e-02,\n",
       "       -2.12890625e-01,  1.12304688e-01, -9.62524414e-02, -6.88476562e-02,\n",
       "        2.32696533e-02, -1.36718750e-02, -8.12988281e-02,  4.65087891e-02,\n",
       "        8.04443359e-02,  1.31347656e-01, -1.07910156e-01,  2.11914062e-01,\n",
       "       -5.34057617e-02, -2.01171875e-01,  1.38061523e-01,  2.51464844e-02,\n",
       "       -6.39648438e-02,  4.18701172e-02, -2.59765625e-01,  5.98144531e-03,\n",
       "        8.41369629e-02, -1.07177734e-01, -1.22070312e-04,  1.88964844e-01,\n",
       "        3.27636719e-01, -2.13378906e-01,  1.27868652e-01, -1.62231445e-01,\n",
       "        3.79638672e-02,  1.36718750e-02,  1.16086960e-01, -2.78320312e-02,\n",
       "        1.29394531e-02,  8.58154297e-02, -1.66503906e-01,  1.06597900e-01,\n",
       "       -1.22070312e-03, -7.05566406e-02,  1.30004883e-02,  9.22393799e-03,\n",
       "       -5.41992188e-02, -1.16943359e-01,  1.43493652e-01,  9.76562500e-02,\n",
       "       -6.54754639e-02,  9.83886719e-02,  4.18090820e-03, -2.43164062e-01,\n",
       "       -6.34765625e-02,  7.93457031e-02, -5.99365234e-02, -1.41601562e-02,\n",
       "        6.83593750e-03, -9.40856934e-02, -1.24511719e-01,  3.54003906e-03,\n",
       "       -8.23974609e-02,  1.40380859e-01,  6.44531250e-02,  2.30468750e-01,\n",
       "       -1.79443359e-01, -4.37011719e-02, -1.85546875e-01, -1.14746094e-01,\n",
       "        2.64648438e-01, -6.33544922e-02, -1.36474609e-01, -5.77392578e-02,\n",
       "       -2.44628906e-01,  1.29150391e-01,  7.08007812e-02,  8.56933594e-02,\n",
       "        4.89501953e-02, -1.82128906e-01,  4.15344238e-02,  1.68457031e-02,\n",
       "       -1.03881836e-01, -4.55322266e-02,  6.25000000e-02,  7.06787109e-02,\n",
       "        6.59179688e-03, -6.72607422e-02,  9.24682617e-02,  2.11791992e-02,\n",
       "       -8.05664062e-03,  8.36181641e-02,  1.90917969e-01,  6.21948242e-02,\n",
       "        1.64550781e-01,  1.30126953e-01,  4.91333008e-02, -6.93359375e-02,\n",
       "       -1.93115234e-01,  5.12695312e-03, -1.58447266e-01,  1.17553711e-01,\n",
       "       -6.68830276e-02, -7.51953125e-02,  7.81250000e-02,  1.45996094e-01,\n",
       "        6.34765625e-03,  1.87500000e-01,  1.27197266e-01, -5.37185669e-02,\n",
       "        4.95605469e-02, -3.03955078e-02, -7.33947754e-02, -1.73828125e-01,\n",
       "        4.90722656e-02, -1.02050781e-01, -8.71582031e-02, -7.16552734e-02,\n",
       "       -1.10839844e-01,  1.08520508e-01,  4.83398438e-02, -1.08154297e-01,\n",
       "       -7.74536133e-02, -4.71191406e-02, -9.57031250e-02, -2.39257812e-02,\n",
       "        2.38769531e-01,  1.34277344e-01,  6.44531250e-02, -9.60693359e-02,\n",
       "       -7.84301758e-02, -2.39257812e-01, -1.86523438e-01,  2.64892578e-02,\n",
       "        7.63244629e-02,  2.90527344e-02,  1.34033203e-01,  1.34765625e-01,\n",
       "       -2.27661133e-02,  1.56250000e-02, -2.39257812e-02, -2.69531250e-01,\n",
       "       -8.16955566e-02,  3.33251953e-02, -2.34375000e-02,  1.66992188e-01,\n",
       "       -1.52343750e-01,  1.90429688e-02, -1.00097656e-01, -7.65380859e-02,\n",
       "       -4.88281250e-04,  4.87670898e-02, -3.66210938e-02, -8.50524902e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word2vec_model[\"really\"] + word2vec_model[\"i\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aa2dc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "96918d0c-209e-4ba0-89e1-f435e57ea4bc",
       "rows": [
        [
         "0",
         "I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10",
         "positive"
        ],
        [
         "1",
         "Not many television shows appeal to quite as many different kinds of fans like Farscape does...I know youngsters and 30/40+ years old;fans both Male and Female in as many different countries as you can think of that just adore this T.V miniseries. It has elements that can be found in almost every other show on T.V, character driven drama that could be from an Australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"Trekkie\" a run for his money in the brainbender stakes! Wormhole theory, Time Travel in true equational form...Magnificent. It embraces cultures from all over the map as the possibilities are endless having multiple stars and therefore thousands of planets to choose from.<br /><br />With such a broad scope; it would be expected that nothing would be able to keep up the illusion for long, but here is where \"Farscape\" really comes into it's own element...It succeeds where all others have failed, especially the likes of Star Trek (a universe with practically zero Kaos element!) They ran out of ideas pretty quickly + kept rehashing them! Over the course of 4 seasons they manage to keep the audience's attention using good continuity and constant character evolution with multiple threads to every episode with unique personal touches to camera that are specific to certain character groups within the whole. This structure allows for an extremely large area of subject matter as loyalties are forged and broken in many ways on many many issues. I happened to see the pilot (Premiere) in passing and just had to keep tuning in after that to see if Crichton would ever \"Get the girl\", after seeing them all on television I was delighted to see them available on DVD & I have to admit that it was the only thing that kept me sane whilst I had to do a 12 hour night shift and developed chronic insomnia...Farscape was the only thing to get me through those extremely long nights...<br /><br />Do yourself a favour; Watch the pilot and see what I mean...<br /><br />Farscape Comet",
         "positive"
        ],
        [
         "2",
         "The film quickly gets to a major chase scene with ever increasing destruction. The first really bad thing is the guy hijacking Steven Seagal would have been beaten to pulp by Seagal's driving, but that probably would have ended the whole premise for the movie.<br /><br />It seems like they decided to make all kinds of changes in the movie plot, so just plan to enjoy the action, and do not expect a coherent plot. Turn any sense of logic you may have, it will reduce your chance of getting a headache.<br /><br />I does give me some hope that Steven Seagal is trying to move back towards the type of characters he portrayed in his more popular movies.",
         "negative"
        ],
        [
         "3",
         "Jane Austen would definitely approve of this one!<br /><br />Gwyneth Paltrow does an awesome job capturing the attitude of Emma. She is funny without being excessively silly, yet elegant. She puts on a very convincing British accent (not being British myself, maybe I'm not the best judge, but she fooled me...she was also excellent in \"Sliding Doors\"...I sometimes forget she's American ~!). <br /><br />Also brilliant are Jeremy Northam and Sophie Thompson and Phyllida Law (Emma Thompson's sister and mother) as the Bates women. They nearly steal the show...and Ms. Law doesn't even have any lines!<br /><br />Highly recommended.",
         "positive"
        ],
        [
         "4",
         "Expectations were somewhat high for me when I went to see this movie, after all I thought Steve Carell could do no wrong coming off of great movies like Anchorman, The 40 Year-Old Virgin, and Little Miss Sunshine. Boy, was I wrong.<br /><br />I'll start with what is right with this movie: at certain points Steve Carell is allowed to be Steve Carell. There are a handful of moments in the film that made me laugh, and it's due almost entirely to him being given the wiggle-room to do his thing. He's an undoubtedly talented individual, and it's a shame that he signed on to what turned out to be, in my opinion, a total train-wreck.<br /><br />With that out of the way, I'll discuss what went horrifyingly wrong.<br /><br />The film begins with Dan Burns, a widower with three girls who is being considered for a nationally syndicated advice column. He prepares his girls for a family reunion, where his extended relatives gather for some time with each other.<br /><br />The family is high atop the list of things that make this an awful movie. No family behaves like this. It's almost as if they've been transported from Pleasantville or Leave it to Beaver. They are a caricature of what we think a family is when we're 7. It reaches the point where they become obnoxious and simply frustrating. Touch football, crossword puzzle competitions, family bowling, and talent shows ARE NOT HOW ACTUAL PEOPLE BEHAVE. It's almost sickening.<br /><br />Another big flaw is the woman Carell is supposed to be falling for. Observing her in her first scene with Steve Carell is like watching a stroke victim trying to be rehabilitated. What I imagine is supposed to be unique and original in this woman comes off as mildly retarded.<br /><br />It makes me think that this movie is taking place on another planet. I left the theater wondering what I just saw. After thinking further, I don't think it was much.",
         "negative"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I really liked this Summerslam due to the look...  positive\n",
       "1  Not many television shows appeal to quite as m...  positive\n",
       "2  The film quickly gets to a major chase scene w...  negative\n",
       "3  Jane Austen would definitely approve of this o...  positive\n",
       "4  Expectations were somewhat high for me when I ...  negative"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/febse/data/raw/refs/heads/main/ta/IMDB-Dataset-5000.csv.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23c8d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_embedding(text, model):\n",
    "    words = [w for w in text.split() if w in model]\n",
    "    if words:\n",
    "        return np.mean([model[w] for w in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df['avg_word'] = df['review'].apply(lambda x: document_embedding(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ac90ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.026841605, 0.044992644, 0.017120501, 0.0777...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.044581123, 0.025324179, 0.020859266, 0.0939...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.04855869, 0.031639244, 0.0031714232, 0.0996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.03325866, 0.024321612, -0.0009411083, 0.060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.056710232, 0.029389769, 0.031232158, 0.0919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  I really liked this Summerslam due to the look...  positive   \n",
       "1  Not many television shows appeal to quite as m...  positive   \n",
       "2  The film quickly gets to a major chase scene w...  negative   \n",
       "3  Jane Austen would definitely approve of this o...  positive   \n",
       "4  Expectations were somewhat high for me when I ...  negative   \n",
       "\n",
       "                                            avg_word  \n",
       "0  [0.026841605, 0.044992644, 0.017120501, 0.0777...  \n",
       "1  [0.044581123, 0.025324179, 0.020859266, 0.0939...  \n",
       "2  [0.04855869, 0.031639244, 0.0031714232, 0.0996...  \n",
       "3  [0.03325866, 0.024321612, -0.0009411083, 0.060...  \n",
       "4  [0.056710232, 0.029389769, 0.031232158, 0.0919...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e286ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert avg_word column to numpy array\n",
    "X = np.vstack(df['avg_word'].values)\n",
    "y = df['sentiment'].map({'positive': 1, 'negative': 0}).values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "158838c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38030644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02684161,  0.04499264,  0.0171205 , ..., -0.06149487,\n",
       "         0.02433256, -0.02856859],\n",
       "       [ 0.04458112,  0.02532418,  0.02085927, ..., -0.01347773,\n",
       "         0.03515555, -0.0291379 ],\n",
       "       [ 0.04855869,  0.03163924,  0.00317142, ..., -0.05177125,\n",
       "         0.04260951, -0.02817884],\n",
       "       [ 0.03325866,  0.02432161, -0.00094111, ..., -0.06046228,\n",
       "         0.06452684, -0.00596103],\n",
       "       [ 0.05671023,  0.02938977,  0.03123216, ..., -0.03327884,\n",
       "         0.05188526, -0.03141383]], shape=(5, 300), dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a070de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ac580",
   "metadata": {},
   "source": [
    "## Getting Document Vectors\n",
    "\n",
    "![Large Language Models](https://bea.stollnitz.com/images/gpt-transformer/3-transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.019143931567668915,\n",
       " -0.025292053818702698,\n",
       " -0.0017211713129654527,\n",
       " 0.01883450709283352,\n",
       " -0.03382139280438423]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"API_KEY_HERE\")\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Get embedding for a text using OpenAI's API (v1.0+ compatible).\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to embed\n",
    "    model (str): The embedding model to use (default: \"text-embedding-3-small\")\n",
    "    \n",
    "    Returns:\n",
    "    list: The embedding vector\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Download embeddings for each review (this may take time and cost money)\n",
    "# Uncomment the line below to run (be aware of API costs)\n",
    "\n",
    "doc_embedding = get_openai_embedding(\"Hello, world!\")\n",
    "doc_embedding[:5]  # Show first 5 dimensions of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de4414e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11026029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6366424560546875,\n",
       " 0.37365591526031494,\n",
       " -1.7717797756195068,\n",
       " 1.5997880697250366,\n",
       " -0.5661576986312866]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_ollama_embedding(text, model=\"llama3.2:latest\"):\n",
    "    \"\"\"\n",
    "    Get embedding for a text using Ollama's local API.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to embed\n",
    "    model (str): The embedding model to use (default: \"nomic-embed-text\")\n",
    "\n",
    "    Returns:\n",
    "    list: The embedding vector\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": text\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"embedding\"]\n",
    "\n",
    "# Example usage:\n",
    "doc_embedding = get_ollama_embedding(\"\"\"\n",
    "I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n",
    "\"\"\")\n",
    "doc_embedding[:5]  # Show first 5 dimensions of the embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
